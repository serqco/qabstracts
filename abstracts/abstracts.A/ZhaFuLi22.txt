Towards Robustness of Deep Program Processing Models - Detection, Estimation, and Enhancement.


Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy).
{{background}}
However, the
robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs,
still lacks systematic investigation as an important quality indicator.
{{gap}}
This article initiates an early step and
proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models
for source code processing.
{{objective}}
We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently.
{{design}}
Based on this, we define the robustness
metrics and propose robustness measurement toolkit CARROTM , which employs the worst-case performance
approximation under the allowable perturbations.
{{design}}
We further propose to improve the robustness of the DL
models by adversarial training (CARROTT ) with our proposed attack techniques.
{{design}}
Our in-depth evaluations
on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM,
ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness
enhancement.
{{method,result:i3}}
---
