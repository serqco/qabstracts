Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings.

Neural program embeddings have demonstrated considerable promise
in a range of program analysis tasks, including clone identification,
program repair, code completion, and program synthesis.
{{}}
However,
most existing methods generate neural program embeddings di-
rectly from the program source codes, by learning from features
such as tokens, abstract syntax trees, and control flow graphs.
{{}}
This paper takes a fresh look at how to improve program embed-
dings by leveraging compiler intermediate representation (IR).
{{}}
We
first demonstrate simple yet highly effective methods for enhancing
embedding quality by training embedding models alongside source
code and LLVM IR generated by default optimization levels (e.g.,
-O2).
{{}}
We then introduce IRGen, a framework based on genetic algo-
rithms (GA), to identify (near-)optimal sequences of optimization
flags that can significantly improve embedding quality.
{{}}
We use IRGen to find optimal sequences of LLVM optimization
flags by performing GA on source code datasets.
{{}}
We then extend a
popular code embedding model, CodeCMR, by adding a new objec-
tive based on triplet loss to enable a joint learning over source code
and LLVM IR.
{{}}
We benchmark the quality of embedding using a rep-
resentative downstream application, code clone detection.
{{}}
When
CodeCMR was trained with source code and LLVM IRs optimized
by findings of IRGen, the embedding quality was significantly im-
proved, outperforming the state-of-the-art model, CodeBERT, which
was trained only with source code.
{{}}
Our augmented CodeCMR also
outperformed CodeCMR trained over source code and IR optimized
with default optimization levels.
{{}}
We investigate the properties of
optimization flags that increase embedding quality, demonstrate
IRGen’s generalization in boosting other embedding models, and
establish IRGen’s use in settings with extremely limited training
data.
{{}}
Our research and findings demonstrate that a straightforward
addition to modern neural code embedding models can provide a
highly effective enhancement.
{{}}
∗ Corresponding   author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page.
{{}}
Copyrights for components of this work owned by others than ACM
must be honored.
{{}}
Abstracting with credit is permitted.
{{}}
To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee.
{{}}
Request permissions from permissions@acm.org.
{{}}
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
{{}}
ACM ISBN 978-1-4503-9221-1/22/05.
{{}}
.
{{}}
. $
{{}}
15.00
https://doi.org/10.1145/3510003.3510217
{{}}
---
