Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding.

With the great success of pre-trained models, the pretrain-then-
finetune paradigm has been widely adopted on downstream tasks
for source code understanding.
{{}}
However, compared to costly train-
ing a large-scale model from scratch, how to effectively adapt pre-
trained models to a new task has not been fully explored.
{{}}
In this
paper, we propose an approach to bridge pre-trained models and
code-related tasks.
{{}}
We exploit semantic-preserving transformation
to enrich downstream data diversity, and help pre-trained models
learn semantic features invariant to these semantically equivalent
transformations.
{{}}
Further, we introduce curriculum learning to or-
ganize the transformed data in an easy-to-hard manner to fine-tune
existing pre-trained models.
{{}}
We apply our approach to a range of pre-trained models, and
they significantly outperform the state-of-the-art models on tasks
for source code understanding, such as algorithm classification,
code clone detection, and code search.
{{}}
Our experiments even show
that without heavy pre-training on code data, natural language pre-
trained model RoBERTa fine-tuned with our lightweight approach
could outperform or rival existing code pre-trained models fine-
tuned on the above tasks, such as CodeBERT and GraphCodeBERT.
{{}}
This finding suggests that there is still much room for improvement
in code pre-trained models.
{{}}
∗ Zhouyang   Jia and Shanshan Li are the corresponding authors.
{{}}
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page.
{{}}
Copyrights for components of this work owned by others than the
author(s) must be honored.
{{}}
Abstracting with credit is permitted.
{{}}
To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee.
{{}}
Request permissions from permissions@acm.org.
{{}}
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s).
{{}}
Publication rights licensed to ACM.
{{}}
ACM ISBN 978-1-4503-9221-1/22/05.
{{}}
.
{{}}
. $
{{}}
15.00
https://doi.org/10.1145/3510003.3510062
{{}}
---
