The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects.

The Impact of Dormant Defects on Defect Prediction:
{{}}
A Study of 19 Apache Projects
DAVIDE FALESSI, University of Rome Tor Vergata, Italy
AALOK AHLUWALIA, California Polytechnic State University, USA
MASSIMILIANO DI PENTA, University of Sannio, Italy
Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has
been the subject of a substantial effort in academia, and some applications in industrial contexts.
{{}}
A necessary
precondition when creating a defect prediction model is the availability of defect data from the history of
projects.
{{}}
If this data is noisy, the resulting defect prediction model could result to be unreliable.
{{}}
One of the
causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction.
{{}}
This can cause a class to be labeled as defect-free while it is not, and is, therefore
“snoring.” In this article, we investigate the impact of snoring on classifiers’ accuracy and the effectiveness
of a possible countermeasure, i.e., dropping too recent data from a training set.
{{}}
We analyze the accuracy of
15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of
19 open source projects from the Apache ecosystem.
{{}}
Our results show that on average across projects (i) the
presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the
training set the classes that in the last release are labeled as not defective significantly improves the accuracy
of the classifiers.
{{}}
In summary, this article provides insights on how to create defects datasets by mitigating
the negative effect of dormant defects on defect prediction.
{{}}
---
