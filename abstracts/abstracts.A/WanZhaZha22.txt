What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code.

Recently, many pre-trained language models for source code have
been proposed to model the context of code and serve as a basis for
downstream code intelligence tasks such as code completion, code
search, and code summarization.
{{background}}
These models leverage masked
pre-training and Transformer and have achieved promising results.
{{background}}
However, currently there is still little progress regarding inter-
pretability of existing pre-trained code models.
{{gap}}
It is not clear why
these models work and what feature correlations they can capture.
{{gap}}
In this paper, we conduct a thorough structural analysis aiming
to provide an interpretation of pre-trained language models for
source code (e.g., CodeBERT, and GraphCodeBERT) from three
distinctive perspectives:
{{objective}}
(1) attention analysis, (2) probing on the
word embedding, and (3) syntax tree induction.
{{objective}}
Through compre-
hensive analysis, this paper reveals several insightful findings that
may inspire future studies:
{{method}}
(1) Attention aligns strongly with the
syntax structure of code.
{{result}}
(2) Pre-training language models of code
can preserve the syntax structure of code in the intermediate rep-
resentations of each Transformer layer.
{{result}}
(3) The pre-trained models
of code have the ability of inducing syntax trees of code.
{{result}}
Theses
findings suggest that it may be helpful to incorporate the syntax
structure of code into the process of pre-training for better code
representations.
{{conclusion}}
---
