Towards Security Threats of Deep Learning Systems: A Survey.

Deep learning has gained tremendous success and great popularity in the past few years.
{{background}}
However, deep learning systems
are suffering several inherent weaknesses, which can threaten the security of learning models.
{{background}}
Deep learning’s wide use further
magnifies the impact and consequences.
{{background}}
To this end, lots of research has been conducted with the purpose of exhaustively identifying
intrinsic weaknesses and subsequently proposing feasible mitigation.
{{background}}
Yet few are clear about how these weaknesses are incurred and
how effective these attack approaches are in assaulting deep learning.
{{gap}}
In order to unveil the security weaknesses and aid in the
development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these
attacks to conclude some findings in multiple views.
{{objective}}
In particular, we focus on four types of attacks associated with security threats of
deep learning:
{{method}}
model extraction attack, model inversion attack, poisoning attack and adversarial attack.
{{method}}
For each type of attack, we
construct its essential workflow as well as adversary capabilities and attack goals.
{{method}}
Pivot metrics are devised for comparing the attack
approaches, by which we perform quantitative and qualitative analyses.
{{method}}
From the analysis, we have identified significant and
indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring
perturbation.
{{result:i1}}
We shed light on 18 findings covering these approaches’ merits and demerits, success probability, deployment complexity
and prospects.
{{result}}
Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research
in this area.
{{result,conclusion}}
---
'objective' sentence that sits on the fence whether this is an empirical paper or a design paper:
"In order to unveil the security weaknesses and aid in the
development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these
attacks to conclude some findings in multiple views."
The title helps, however, to decide it is empirical ("a survey").