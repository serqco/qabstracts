An Empirical Study of the Impact of Hyperparameter Tuning and Model Optimization on the Performance Properties of Deep Neural Networks.

An Empirical Study of the Impact of Hyperparameter Tuning
and Model Optimization on the Performance Properties of
Deep Neural Networks
LIZHI LIAO, Concordia University, Canada
HENG LI, Polytechnique Montréal, Canada
WEIYI SHANG, Concordia University, Canada
LEI MA, University of Alberta, Canada
Deep neural network (DNN) models typically have many hyperparameters that can be configured to
achieve optimal performance on a particular dataset.
{{}}
Practitioners usually tune the hyperparameters of their
DNN models by training a number of trial models with different configurations of the hyperparameters, to
find the optimal hyperparameter configuration that maximizes the training accuracy or minimizes the training loss.
{{}}
As such hyperparameter tuning usually focuses on the model accuracy or the loss function, it is
not clear and remains under-explored how the process impacts other performance properties of DNN models, such as inference latency and model size.
{{}}
On the other hand, standard DNN models are often large in
size and computing-intensive, prohibiting them from being directly deployed in resource-bounded environments such as mobile devices and Internet of Things (IoT) devices.
{{}}
To tackle this problem, various model
optimization techniques (e.g., pruning or quantization) are proposed to make DNN models smaller and less
computing-intensive so that they are better suited for resource-bounded environments.
{{}}
However, it is neither
clear how the model optimization techniques impact other performance properties of DNN models such as
inference latency and battery consumption, nor how the model optimization techniques impact the effect
of hyperparameter tuning (i.e., the compounding effect).
{{}}
Therefore, in this paper, we perform a comprehensive study on four representative and widely-adopted DNN models, i.e., CNN image classification, Resnet-50,
CNN text classification, and LSTM sentiment classification, to investigate how different DNN model hyperparameters affect the standard DNN models, as well as how the hyperparameter tuning combined with model
optimization affect the optimized DNN models, in terms of various performance properties (e.g., inference
latency or battery consumption).
{{}}
Our empirical results indicate that tuning specific hyperparameters has heterogeneous impact on the performance of DNN models across different models and different performance
properties.
{{}}
In particular, although the top tuned DNN models usually have very similar accuracy, they may
have significantly different performance in terms of other aspects (e.g., inference latency).
{{}}
We also observe
that model optimization has a confounding effect on the impact of hyperparameters on DNN model performance.
{{}}
For example, two sets of hyperparameters may result in standard models with similar performance
but their performance may become significantly different after they are optimized and deployed on the mobile
device.
{{}}
Our findings highlight that practitioners can benefit from paying attention to a variety of performance
properties and the confounding effect of model optimization when tuning and optimizing their DNN models.
{{}}
Authors’ addresses:
{{}}
L.
{{}}
Liao and W.
{{}}
Shang, Department of Computer Science and Software Engineering, Concordia University, 1455 Boulevard de Maisonneuve West, Montréal, Québec, Canada H3G 1M8; emails:
{{}}
{l_lizhi, shang}@encs.concordia.ca;
H.
{{}}
Li, Département de génie informatique et génie logiciel, Polytechnique Montréal, 2500 Chemin de Polytechnique, Montréal, Québec, Canada H3T 1J4; email:
{{}}
heng.li@polymtl.ca; L.
{{}}
Ma, Department of Electrical & Computer Engineering, University of Alberta, 116 St.
{{}}
and 85 Ave., Edmonton, Alberta, Canada T6G 2R3; email:
{{}}
ma.lei@acm.org.
{{}}
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page.
{{}}
Copyrights for components of this work owned by others than the author(s) must be
honored.
{{}}
Abstracting with credit is permitted.
{{}}
To copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
{{}}
Request permissions from permissions@acm.org.
{{}}
© 2022 Copyright held by the owner/author(s).
{{}}
Publication rights licensed to ACM.
{{}}
1049-331X/2022/04-ART53 $15.00
https://doi.org/10.1145/3506695
ACM Transactions on Software Engineering and Methodology, Vol.
{{}}
31, No.
{{}}
3, Article 53.
{{}}
Pub.
{{}}
date:
{{}}
April 2022.
{{}}
53
{{}}
---
