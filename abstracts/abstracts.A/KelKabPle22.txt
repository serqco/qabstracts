What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning.

What You See is What it Means!
{{cruft}}
Semantic Representation
Learning of Code based on Visualization and Transfer
Learning
PATRICK KELLER, University of Luxembourg
ABDOUL KADER KABORÉ, University of Luxembourg, CITADEL at Université Virtuelle du
Burkina Faso

LAURA PLEIN, Saarland University
JACQUES KLEIN, YVES LE TRAON, and TEGAWENDÉ F.
{{cruft}}
BISSYANDÉ,
University of Luxembourg
{{cruft}}
Recent successes in training word embeddings for Natural Language Processing (NLP) tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods.
{{background}}
The overall objective is then to produce code embeddings that capture the maximum of program semantics.
{{background}}
State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract
syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable.
{{background}}
In this work, we investigate a novel embedding approach based
on the intuition that source code has visual patterns of semantics.
{{objective}}
We further use these patterns to address
the outstanding challenge of identifying semantic code clones.
{{objective}}
We propose the WySiWiM (‘‘What You See
Is What It Means”) approach where visual representations of source code are fed into powerful pre-trained
image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning.
{{design}}
We evaluate the proposed embedding approach on the task of vulnerable code
prediction in source code and on two variations of the task of semantic code clone identification:
{{method}}
code clone
detection (a binary classification problem), and code classification (a multi-classification problem).
{{method}}
We show
with experiments on the BigCloneBench (Java), Open Judge (C) that although simple, our WySiWiM approach
performs as effectively as state-of-the-art approaches such as ASTNN or TBCNN.
{{method,result:i2}}
We also showed with data
from NVD and SARD that WySiWiM representation can be used to learn a vulnerable code detector with
reasonable performance (accuracy ∼90%).
{{result}}
We further explore the influence of different steps in our approach,
such as the choice of visual representations or the classification algorithm, to eventually discuss the promises
and limitations of this research direction.
{{a-conclusion}}
---
