Deep Just-In-Time Defect Localization.

During software development and maintenance, defect localization is an essential part of software quality assurance.
{{background}}
Even
though different techniques have been proposed for defect localization, i.e., information retrieval (IR)-based techniques and spectrumbased techniques, they can only work after the defect has been exposed, which can be too late and costly to adapt to the newly
introduced bugs in the daily development.
{{background}}
There are also many JIT defect prediction tools that have been proposed to predict the buggy
commit.
{{background}}
But these tools do not locate the suspicious buggy positions in the buggy commit.
{{background}}
To assist developers to detect bugs in time
and avoid introducing them, just-in-time (JIT) bug localization techniques have been proposed, which is targeting to locate suspicious
buggy code after a change commit has been submitted.
{{background}}
In this paper, we propose a novel JIT defect localization approach, named
DEEPDL (Deep Learning-based defect localization), to locate defect code lines within a defect introducing change.
{{objective}}
DEEPDL employs a
neural language model to capture the semantics of the code lines, in this way, the naturalness of each code line can be learned and
converted to a suspiciousness score.
{{design}}
The core of our DEEPDL is a deep learning-based neural language model.
{{design}}
We train the neural
language model with previous snapshots (history versions) of a project so that it can calculate the naturalness of a piece of code.
{{design}}
In its
application, for a given new code change, DEEPDL automatically assigns a suspiciousness score to each code line and sorts these code
lines in descending order of this score.
{{design}}
The code lines at the top of the list are considered as potential defect locations.
{{design}}
Our tool can
assist developers efficiently check buggy lines at an early stage, which is able to reduce the risk of introducing bugs in time and improve
the developers' confidence in the reliability of their software.
{{claim}}
We conducted an extensive experiment on 14 open source Java projects
with a total of 11,615 buggy changes.
{{method}}
We evaluate the experimental results considering four evaluation metrics.
{{method:i1}}
The experimental
results show that our method outperforms the state-of-the-art by a substantial margin.
{{result:i2}}
---
