SE3M: A model for software effort estimation using pre-trained embedding models.

Context:
{{}}
Software effort estimation from requirements texts, presents many challenges, mainly in getting
viable features to infer effort.
{{}}
The most recent Natural Language Processing (NLP) initiatives for this purpose
apply context-less embedding models, which are often not sufficient to adequately discriminate each analyzed
sentence.
{{}}
Contextualized pre-trained embedding models have emerged quite recently and have been shown to
be far more effective than context-less models in representing textual features.
{{}}
Objective:
{{}}
This paper proposes evaluating the effectiveness of pre-trained embedding models, to explore a
more effective technique for representing textual requirements, which are used to infer effort estimates by
analogy.
{{}}
Method:
{{}}
Generic pre-trained models went through a fine-tuning process for both approaches â€” context-less
and contextualized.
{{}}
The generated models were used as input in the applied deep learning architecture, with
linear output.
{{}}
The results were very promising, realizing that contextualized pre-trained embedding models
can be used to estimate software effort based only on requirements texts.
{{}}
Results:
{{}}
We highlight the results obtained to apply the contextualized pre-trained model BERT with finetuning, applied in a single repository containing different projects, whose Mean Absolute Error (MAE) value is
4.25 and the standard deviation is only 0.17.
{{}}
This represents a result very positive when compared to similar
works.
{{}}
Conclusion:
{{}}
The main advantages of the proposed estimation method are reliability, the possibility of
generalization, speed, and low computational cost.
{{}}
Such advantages are provided by the fine-tuning process,
enabling to infer effort estimation for new or existing requirements.
{{}}
---
