Transfer Learning Across Variants and Versions: The Case of Linux Kernel Size.

With large scale and complex configurable systems, it is hard for users to choose the right combination of options (i.e.,
configurations) in order to obtain the wanted trade-off between functionality and performance goals such as speed or size.
{{}}
Machine
learning can help in relating these goals to the configurable system options, and thus, predict the effect of options on the outcome,
typically after a costly training step.
{{}}
However, many configurable systems evolve at such a rapid pace that it is impractical to retrain a
new model from scratch for each new version.
{{}}
In this paper, we propose a new method to enable transfer learning of binary size
predictions among versions of the same configurable system.
{{}}
Taking the extreme case of the Linux kernel with its  14; 500
configuration options, we first investigate how binary size predictions of kernel size degrade over successive versions.
{{}}
We show that the
direct reuse of an accurate prediction model from 2017 quickly becomes inaccurate when Linux evolves, up to a 32% mean error by
August 2020.
{{}}
We thus propose a new approach for transfer evolution-aware model shifting (TEAMS).
{{}}
It leverages the structure of a
configurable system to transfer an initial predictive model towards its future versions with a minimal amount of extra processing for each
version.
{{}}
We show that TEAMS vastly outperforms state of the art approaches over the 3 years history of Linux kernels, from 4.13 to 5.8.
{{}}
---
