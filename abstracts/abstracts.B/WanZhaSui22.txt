Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention.

Code summarization (aka comment generation) provides a high-level natural language description of the function
performed by code, which can benefit the software maintenance, code categorization and retrieval.
{{background}}
To the best of our knowledge, the
state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it
into a natural language space.
{{background}}
Such approaches suffer from the following drawbacks:
{{background}}
(a) they are mainly input by representing code as
a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the
features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict
subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate
the entire word sequence from scratch.
{{background}}
As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy.
{{gap}}
To
address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by
incorporating multiple code features, including type-augmented abstract syntax trees and program control flows.
{{objective}}
Such features, along
with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment
generation.
{{design}}
Our approach assigns weights (pays "attention"ï¿½) to tokens and statements when constructing the code representation to
reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees).
{{design}}
Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network,
where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network
computes the reward values of all the possible extensions of the current state to provide global guidance for explorations.
{{design}}
Eventually,
we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset.
{{design,method:i1}}
The experimental
results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-ofthe-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.
{{result}}
---
