A machine and deep learning analysis among SonarQube rules, product, and process metrics for fault prediction.

Background Developers spend more time fixing bugs refactoring the code to increase the
maintainability than developing new features.
{{}}
Researchers investigated the code quality
impact on fault-proneness, focusing on code smells and code metrics.
{{}}
Objective We aim at advancing fault-inducing commit prediction using different variables,
such as SonarQube rules, product, process metrics, and adopting different techniques.
{{}}
Method We designed and conducted an empirical study among 29 Java projects analyzed
with SonarQube and SZZ algorithm to identify fault-inducing and fault-fixing commits,
computing different product and process metrics.
{{}}
Moreover, we investigated fault-proneness
using different Machine and Deep Learning models.
{{}}
Results We analyzed 58,125 commits containing 33,865 faults and infected by more than
174 SonarQube rules violated 1.8M times, on which 48 software product and process metrics
were calculated.
{{}}
Results clearly identified a set of features that provided a highly accurate
fault prediction (more than 95% AUC).
{{}}
Regarding the performance of the classifiers, Deep
Learning provided a higher accuracy compared with Machine Learning models.
{{}}
Conclusion Future works might investigate whether other static analysis tools, such as FindBugs or Checkstyle, can provide similar or different results.
{{}}
Moreover, researchers might
consider the adoption of time series analysis and anomaly detection techniques.
{{}}
---
