Automated Testing of Software that Uses Machine Learning APIs.

An increasing number of software applications incorporate machine
learning (ML) solutions for cognitive tasks that statistically mimic
human behaviors.
{{background}}
To test such software, tremendous human effort
is needed to design image/text/audio inputs that are relevant to the
software, and to judge whether the software is processing these
inputs as most human beings do.
{{background}}
Even when misbehavior is exposed,
it is often unclear whether the culprit is inside the cognitive ML
API or the code using the API.
{{background}}
This paper presents Keeper, a new testing tool for software that
uses cognitive ML APIs.
{{objective}}
Keeper designs a pseudo-inverse function
for each ML API that reverses the corresponding cognitive task in
an empirical way (e.g., an image search engine pseudo-reverses the
image-classification API), and incorporates these pseudo-inverse
functions into a symbolic execution engine to automatically gener-
ate relevant image/text/audio inputs and judge output correctness.
{{design}}
Once misbehavior is exposed, Keeper attempts to change how ML
APIs are used in software to alleviate the misbehavior.
{{design}}
Our evalu-
ation on a variety of open-source applications shows that Keeper
greatly improves the branch coverage, while identifying many pre-
viously unknown bugs.
{{method,result:i2}}
---
