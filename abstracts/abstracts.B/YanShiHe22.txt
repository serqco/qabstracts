Natural Attack for Pre-trained Models of Code.

Pre-trained models of code have achieved success in many impor-
tant software engineering tasks.
{{background}}
However, these powerful models
are vulnerable to adversarial attacks that slightly perturb model
inputs to make a victim model produce wrong outputs.
{{background}}
Current
works mainly attack models of code with examples that preserve op-
erational program semantics but ignore a fundamental requirement
for adversarial example generation:
{{gap}}
perturbations should be natural
to human judges, which we refer to as naturalness requirement.
{{gap}}
In this paper, we propose ALERT (Naturalness Aware Attack),
a black-box attack that adversarially transforms inputs to make
victim models produce wrong outputs.
{{objective}}
Different from prior works,
this paper considers the natural semantic of generated examples
at the same time as preserving the operational semantic of original
inputs.
{{design}}
Our user study demonstrates that human developers consis-
tently consider that adversarial examples generated by ALERT are
more natural than those generated by the state-of-the-art work by
Zhang et al.
that ignores the naturalness requirement.
{{method,result}}
On attack-
ing CodeBERT, our approach can achieve attack success rates of
53.62%, 27.79%, and 35.78% across three downstream tasks:
{{method,result}}
vulnera-
bility prediction, clone detection and code authorship attribution.
{{method}}
On GraphCodeBERT, our approach can achieve average success
rates of 76.95%, 7.96% and 61.47% on the three tasks.
{{result}}
The above
outperforms the baseline by 14.07% and 18.56% on the two pre-
trained models on average.
{{result}}
Finally, we investigated the value of the
generated adversarial examples to harden victim models through
an adversarial fine-tuning procedure and demonstrated the accu-
racy of CodeBERT and GraphCodeBERT against ALERT-generated
adversarial examples increased by 87.59% and 92.32%, respectively.
{{method,result}}
---
