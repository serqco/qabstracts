---
title: Sanity Check
output:
    md_document:
        variant: gfm
        number_sections: true
---

This document summarizes a number of quality issues regarding the data processing and analysis.
They were discovered while attempting to replace the hard-coded values in the `.tex` file with inline-expressions
(Knitr's `\Sexpr{}`), which required identifying the relevant parts in the Python datasets.

# Load Data and Run Python Statistics

```{r}
## This is R
library(reticulate)
# Use R's reticulate to load Python modules (import qabs.dataframes as dataframes)
py$dataframes <- import_from_path("qabs.dataframes", path = ".")
```

```{python}
## This is Python
import pandas as pd

# Load data
datafile = "../results/abstracts-results.tsv"
raw = pd.read_csv(datafile, sep='\t')

# Perform the statistics
datasets = dataframes.create_all_datasets(raw)
dataframes.create_all_subsets(datasets)
```

```{r, message=FALSE}
## This is R
library(dplyr)
library(tidyr)

# Make Python variables available to R, so they can be used in the following document
for (var in names(py)) {
  assign(var, py[[var]], envir = .GlobalEnv)
}

raw <- raw |> as_tibble()
```

## Simple Statistics and A Glimpse into the Structure
```{r}
(n_abstracts_raw <- n_distinct(raw$citekey))
(n_abstracts_py  <- n_distinct(datasets$by_abstract$`_citekey`))
```

```{r}
raw |> head(1) |> t()
datasets$by_abstract_coding |> head(1) |> t()
datasets$by_abstract |> head(1) |> t()
```

# Issues that cause wrong numbers

## Problem: Unclear calculation of `ignorediffs` (RESOLVED)

There was some issue with the calculation of the `ignorediffs` column.

**Example:** `AtaMasHem22` has 2 sentences which Lutz annotated with `-ignorediff`.
In the Python dataset, however, the numbers 4 (for Lutz) and 2 (for Lloyd) showed up.

However, this issues appears to be resolved as of now:

```{r}
# The raw data (codings)
raw |> filter(citekey == "AtaMasHem22") |>
    filter("ignorediff" %in% code, .by = sidx) |>
    select(citekey, coder, codername, sidx, words, code) |>
    arrange(codername, sidx)
# Individual abstract-level codings
datasets$by_abstract_coding |>
    filter(`_citekey` == 'AtaMasHem22') |>
    select(`_citekey`, `_coder`, codername, ignorediffs)
# Abstract-level aggregation
datasets$by_abstract |>
    filter(`_citekey` == 'AtaMasHem22') |>
    select(`_citekey`, ignorediffs)
```

The numbers are summed up correctly and the aggregated count is also correct.


## Problem: Different Word Counts (RESOLVED)
Several abstracts used to differ in their word count.

This appears to be no longer an issue, as the biggest difference are due to rounding errors
(one coder give 2 codes, the other 3 -> sentence length is divided by 2 and by 3, rounded to one decimal place -> sums are 0.1 off).

```{r}
assert_coder_sameness <- function(field) {
    with_diff <- datasets$by_abstract_coding |>
        filter(min({{field}}) != max({{field}}), .by = `_citekey`) |>
        select(`_citekey`, `_coder`, {{field}})
    if (nrow(with_diff) == 0) return ("No differences")
    with_diff |>
        pivot_wider(id_cols = `_citekey`, names_from = `_coder`, values_from = {{field}}) |>
        mutate(diff = abs(A - B)) |>
        arrange(desc(diff))
}

assert_coder_sameness(words)
```

Let's check all the other fields that should not be different between coders A and B:

```{r}
assert_coder_sameness(venue)
assert_coder_sameness(volume)
assert_coder_sameness(sentences)
assert_coder_sameness(chars)
assert_coder_sameness(syllables)
assert_coder_sameness(avg_wordlength)
```

Note: `fkscore` should also be same -- see next problem.


## Problem: Different `fkscore`s
A few abstracts have different `fkscore`s:
```{r}
assert_coder_sameness(fkscore)
```
I have no idea what might cause this.


## Problem: Calculation of `fkscore` ignores multi-codings (RESOLVED)

Abstract-level `fkscore` used to be calculated by taking the mean of sentences' `fkscore`,
which was off for double (or triple) coded sentences.
This is no longer an issue, since the calculation is now done for the whole abstract.


## Problem: Improper Calculation of Fresch-Kincaid (RESOLVED)

Abstract-level `fkscore` used to be calculated by taking the mean of sentences' `fkscore`.
This is no longer an issue, since the calculation is now done for the whole abstract.

To illustrate the issue nevertheless, consider the first four sentences of `CheHuWei22`:

```
Source code summarization is a crucial yet far from settled task for describing structured code
snippets in natural language.
High-quality code summaries could effectively facilitate program comprehension
and software maintenance.
A good code summary is supposed to have the following characteristics:
complete
information, correct meaning, and consistent description.
```

Calculating the FRES for each sentence individually (setting _total sentences := 1_) and calculating the average gives:

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/bd4916e193d2f96fa3b74ee258aaa6fe242e110e)

```{r}
example <- raw |> filter(citekey == "CheHuWei22", coder == "A", sidx >= 2, sidx <= 5) |>
    select(words, syllables, fkscore) |> # Note: requires "syllables" column in raw data!
    mutate(words = words + 1) |> # Fixing the word count -- see next problem!
    mutate(fres = 206.835 - 1.015 * words / 1 - 84.6 * (syllables / words))
example
mean(example$fres)
```

But using the formula properly and taking all sentences into account,
the overall score is:

```{r}
example |>
    summarize(fres = 206.835 - 1.015 * (sum(words) / n()) - 84.6 * (sum(syllables) / sum(words)))
```


## Problem: Improper Word Count (RESOLVED)

Word counts where off in the past:
Consider a sentence with 31 words with 1, 2, or 3 codes.

| Codings | Correct Word Count | Old Word Count |
|---------|--------------------|----------------|
| 1       | 1 x 31             | 1 x 30         |
| 2       | 2 x 15.5           | 2 x 30         |
| 3       | 3 x 10.3           | 3 x 15         |


This appears to be no issue anymore.
As an example, consider sentence ID 8 from `LiuLiFu22` (including its three codes) looks like this:

```
Experimen-
tal results on java methods show that our model can outperform
the state-of-the-art results by a large margin on method name sug-
gestion, demonstrating the effectiveness of our proposed model.
{{a-method,result:i1,conclusion}}
```

The logic in `qscript/annotations.py` calculates the following values:

- words: 31
- syllables: 58
- chars: 204

The export logic in `qabs/export.py` however, creates the following three entries:

```{r}
raw |> filter(citekey == "LiuLiFu22", coder == "B", sidx == 8) |>
    select(citekey, sidx, words, chars, syllables, code, suffixes, topic)
```

Which is correct.


# Issues related to Double Coding of Abstracts
The following issues do not lead to _wrong_ number per se, but to questionable ones.

## I-Gaps and U-Gaps
Each abstract has two `icount` and `ucount` values, which are often not the same:

```{r}
datasets$by_abstract_coding |>
    filter(min(icount) != max(icount) | min(ucount) != max(ucount), .by = `_citekey`) |>
    select(`_citekey`, `_coder`, icount, ucount) |>
    pivot_wider(id_cols = `_citekey`, names_from = `_coder`, values_from = c(icount, ucount))
```

A naive computation of ratios of abstracts with either type of gap ignores this (this is what appears
to be happening for the plots, e.g., Figure 10 in the paper, with the red bar around 55%):

```{r}
sum(datasets$by_abstract$icount >= 1) / nrow(datasets$by_abstract) * 100
```

A better computation would to take the two rating into account,
requiring that both coders saw a gap (`i_min`), at least one saw a gap (`i_max`),
or they saw at least one gap on average (`i_mean`, e.g., 0 and 2 gaps, respectively).
This leads to different percentages:

```{r}
datasets$by_abstract_coding |>
    group_by(`_citekey`) |>
    summarize(i_min = min(icount), i_mean = mean(icount), i_max = max(icount)) |>
    summarize(across(c(i_min, i_mean, i_max), ~sum(. >= 1) / n() * 100)) |>
    t()
```

## Completeness and Properness
There are a number of paper for which the coders disagree whether they are _complete_ or disagree whether they are _proper_.

```{r}
datasets$by_abstract_coding |>
    filter(sum(is_complete) == 1 | sum(is_proper) == 1, .by = `_citekey`) |>
    select(`_citekey`, `_coder`, is_complete, is_proper) |>
    pivot_wider(id_cols = `_citekey`, names_from = `_coder`, values_from = c(is_complete, is_proper))
```
