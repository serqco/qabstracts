# qabstracts: Quality of abstracts study

created: Lutz Prechelt, 2022-08-31 morning
changed: Lutz Prechelt, 2022-08-31 afternoon

This file was used as a kind of presentation to introduce the
current ideas for the study design to
Martin Shepperd, Lloyd Montgomery, Gunnar Bergersen.
The following section was added afterwards:


## post-hoc: Meeting results

We talked for 55 minutes.
- We agreed on the basic approach as proposed
- We will add a code {{limitations}}
- We will add a code {{hype}} to be used as an additional attribute
  of a sentence (not an overall classification).
- All four of us will to be coders in this study. Others are welcome.
- We will all code a first batch (or two) of 10 abstracts and
  each try to come up with some rule for counting pieces of
  relevant information in a sentence. Then discuss those ideas.
  (Lutz will organize this.)


## Overall design of study

1. obtain 1 year each of ICSE, TSE, EMSE, TOSEM, IST
2. draw a random sample of 50 (or 100?) articles from each
3. extract (mostly automatically) the abstract from each article,
   put it in a separate file, with each sentence beginning on a new line.
4. make two copies, called A and B, of each abstract file.
   Two researchers will separately annotate A and B, respectively.
   (More on annotation below; this is our key difficulty.)
5. Compare A and B for equivalence.
   Correct mistakes, but leave true disagreements in.
6. Evaluate statistically:
   - types of abstracts (clustering into 2-5 abstracts styles)
   - abstracts quality overall,
     typical patterns of good and not-so-good abstracts,
     abstracts quality per venue
7. Critique the findings:
   - Are we doing as well as we should? Where and how do we not?
   - Suggest practices for CfPs, reviewing, acceptance decisions


## The codebook

Coding granularity is 'sentence'.

The basic idea is to classify per the parts of a structured abstract,
(background, objective, method, results, conclusion)
plus a code for artifact descriptions in constructive (non-empirical) articles
plus some codes for things that may be interesting and easy to discriminate
(e.g. future work, non-empirical claim).


## Key open codebook problem (with examples of abstracts)

A good abstract should be rich in _relevant_ information.
I believe we need to coarsely quantify the amount of relevant information.
But how?
Preliminary idea: Count number of new, relevant, elementary facts.
But what is that and how to operationalize it repeatably?


========================================================================
### Example 1: A good abstract

Very informative and easily readable:

Ambiguity in natural-language requirements is a
pervasive issue that has been studied by the requirements en-
gineering community for more than two decades.
{{background}}
A fully manual
approach for addressing ambiguity in requirements is tedious
and time-consuming, and may further overlook unacknowledged
ambiguity – the situation where different stakeholders perceive
a requirement as unambiguous but, in reality, interpret the
requirement differently.
{{background}}
In this paper, we propose an automated
approach that uses natural language processing for handling
ambiguity in requirements.
{{objective,design}}
Our approach is based on the au-
tomatic generation of a domain-specific corpus from Wikipedia.
{{design}}
Integrating domain knowledge, as we show in our evaluation,
leads to a significant positive improvement in the accuracy of
ambiguity detection and interpretation.
{{result}}
We scope our work to
coordination ambiguity (CA) and prepositional-phrase attach-
ment ambiguity (PAA) because of the prevalence of these types
of ambiguity in natural-language requirements [1].
{{objective}}
We evaluate
our approach on 20 industrial requirements documents.
{{method}}
These
documents collectively contain more than 5000 requirements
from seven distinct application domains.
{{method}}
Over this dataset, our
approach detects CA and PAA with an average precision of
≈80% and an average recall of ≈89% (≈90% for cases of unac-
knowledged ambiguity).
{{result}}
The automatic interpretations that our
approach yields have an average accuracy of ≈85%.
{{result}}
Compared
to baselines that use generic corpora, our approach, which uses
domain-specific corpora, has ≈33% better accuracy in ambiguity
detection and ≈16% better accuracy in interpretation.
{{result}}
----
#und-high
#inf-high
Very natural train-of-thoughts, lots of concrete information.


========================================================================
### Example 2: An not-so-good abstract

Too much 'background'; announces instead of summarizing:

The SZZ algorithm for identifying bug-inducing
changes has been widely used to evaluate defect prediction
techniques and to empirically investigate when, how, and by
whom bugs are introduced.
{{background}}
Over the years, researchers have pro-
posed several heuristics to improve the SZZ accuracy, providing
various implementations of SZZ.
{{background}}
However, fairly evaluating those
implementations on a reliable oracle is an open problem:
{{background}}
SZZ
evaluations usually rely on (i) the manual analysis of the SZZ
output to classify the identified bug-inducing commits as true or
false positives; or (ii) a golden set linking bug-fixing and bug-
inducing commits.
{{background}}
In both cases, these manual evaluations are
performed by researchers with limited knowledge of the studied
subject systems.
{{background}}
Ideally, there should be a golden set created by
the original developers of the studied systems.
{{background}}
We propose a methodology to build a “developer-informed”
oracle for the evaluation of SZZ variants.
{{objective}}
We use Natural Lan-
guage Processing (NLP) to identify bug-fixing commits in which
developers explicitly reference the commit(s) that introduced a
fixed bug.
{{design}}
This was followed by a manual filtering step aimed at
ensuring the quality and accuracy of the oracle.
{{design}}
Once built, we
used the oracle to evaluate several variants of the SZZ algorithm
in terms of their accuracy.
{{method}}
Our evaluation helped us to distill a
set of lessons learned to further improve the SZZ algorithm.
{{announce-result}}
----
#und-high
#inf-low
Over 50% background, remainder is vague.


========================================================================


