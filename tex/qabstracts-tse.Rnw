% Article about SE abstracts quality (qabstracts) for IEEE Trans. on Software Engineering
% \Todo:
% - decide which part of figure discussion belongs into caption and which into the body text
% - https://bjoern.brembs.net/2016/01/even-without-retractions-top-journals-publish-the-least-reliable-science/
% - check consistent use of \Cb, \Sah, \Art and of \Quote, \Pseudoquote, \QuoteCB
% - discuss pointers from https://statistically-funny.blogspot.com/2023/06/a-study-in-misplaced-scientific-flair.html
% https://absolutelymaybe.plos.org/2023/12/31/5-things-we-learned-about-peer-review-in-2023/#5

\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage{cite}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\usepackage{stfloats}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{enumitem}  % For letter-based lists
\usepackage{listings}  % To show code/text as an example

\usepackage{booktabs}

\graphicspath{ {../img} {.} }
\usepackage{xcolor}
\usepackage{balance}
\hyphenation{semi-conduc-tor IEEE-Xplore}

\newcommand{\ifarxiv}[1]{#1}  % activate this for the ArXiv version of the document
%\newcommand{\ifarxiv}[1]{}  % activate this for the journal version of the document

% ----- Custom Plot Commands to Control Shared Attributes
% Most generic shared attributes
\newcommand{\PlotCustom}[5]{%
	\begin{#1}[#3]%
		\centering\includegraphics[width=#4]{#2}%
		\vspace{-4mm}\caption{#5}\label{#2}%
	\end{#1}}
% Subclassing within-column figures
\newcommand{\Plot}[2]{\PlotCustom{figure}{#1}{htb}{\columnwidth}{#2}}
\newcommand{\PlotWidth}[3]{\PlotCustom{figure}{#1}{htb}{#2\columnwidth}{#3}}
% Subclassing across-column figures
\newcommand{\Plotwide}[2]{\PlotCustom{figure*}{#1}{tbp}{\textwidth}{#2}}
\newcommand{\PlotwideWidth}[3]{\PlotCustom{figure*}{#1}{tbp}{#2\textwidth}{#3}}
% NOTE: These "classes" could be further coupled as "subclasses" to reduce duplicate code, but I believe this makes it harder to trace their individual behaviour.

\newcommand{\Cb}[1]{\bgroup\mbox{\scshape #1}\egroup}  % typeset a code name
\newcommand{\Prg}[1]{\bgroup\ttfamily #1\egroup}  % typeset commands and script names
\newcommand{\strct}[1]{\bgroup\ttfamily\bfseries #1\egroup}  % typeset an abstract structure code
\newcommand{\Art}[1]{\bgroup[#1]\egroup} % typeset article references
\newcommand{\Sah}[1]{\bgroup\scshape #1\egroup}  % typeset a structured abstract heading

\newcounter{todonumber}
\newcommand{\Todo}[1]{\stepcounter{todonumber}\textcolor{red}{\sffamily ((\arabic{todonumber}: #1))}}
\newcommand{\Quote}[1]{\bgroup\itshape ``#1''\egroup}  % quote from real abstract
\newcommand{\Pseudoquote}[1]{\bgroup\itshape `#1'\egroup}  % quote from fictive abstract
\newcommand{\QuoteCB}[1]{\Quote{#1}}  % quote from codebook
\newcommand{\Describeboxplots}{The box shows the 25-to-75 percentile, 
	the whiskers are 10- and 90-percentile,
	the grey bar is the median,
	the fat dot is the mean.}
\newcommand{\Describegroups}{The plots in each group show these different subsets of abstracts:
	all (red); structured, non-structured (green); design, empirical (blue); EMSE, ICSE, IST, TOSEM, TSE (grey).}

\begin{document}

\title{How (Not) To Write a\\Software Engineering Abstract}

\author{Lutz Prechelt%
\thanks{L. Prechelt is with Freie Universit√§t Berlin, Germany},
Lloyd Montgomery%
\thanks{L. Montgomery is with University of Hamburg, Germany},  % Our university prefers the English name in publications
Julian Frattini, Franz Zieris%
\thanks{J. Frattini and F. Zieris are with BTH, Karlskrona, Sweden}}

\markboth{IEEE Transactions on Software Engineering}%
{How (Not) To Write a Software Engineering Abstract}

\maketitle

<<setup-knitr-reticulate, echo=FALSE>>=
################ This is R code ################
library(reticulate)
# Use R's reticulate to load Python modules; read it as:
# ```python
# import qabs.dataframes as dataframes
# ```
py$dataframes <- import_from_path("qabs.dataframes", path = "../script")

options(width = 50)
debug_knitr <- "debug" %in% commandArgs(TRUE)
knitr::opts_chunk$set(
    echo    = ifelse(debug_knitr, TRUE, FALSE),      # TRUE:     see all R code
    results = ifelse(debug_knitr, "markup", "hide"), # 'markup': see intermediate values
    size    = "scriptsize"
) 
@

<<load-data, engine='python', echo=FALSE>>=
################### This is Python code ################
import pandas as pd

# Load data (output of `make export`)
datafile = "../results/abstracts-results.tsv"
raw = pd.read_csv(datafile, sep='\t')

# Perform the statistics
datasets = dataframes.create_all_datasets(raw)
dataframes.create_all_subsets(datasets)
@

<<import-data, echo=FALSE, message=FALSE>>=
################ This is R code ################
library(dplyr)
library(tidyr)
library(stringr)
library(glue)
library(kableExtra)

# Make Python variables available to R, so they can be used in the following document
for (var in names(py)) {
  assign(var, py[[var]], envir = .GlobalEnv)
}

# We use this dataset the most, so we provide a shorthand
df   <- datasets$by_abstract
df_c <- datasets$by_abstract_coding

# Percentages occur often and need to be properly escaped in LaTeX
format_percent <- function(value, accuracy = 1) {
  scales::label_percent(accuracy)(value |> as.numeric()) |> as.character() |> stringr::str_replace("%", "\\\\%")
}

format_p_value <- function(p) {
  case_when(
    p < 0.001 ~ "p<0.001",
    p < 0.01  ~ "p<0.01",
    p < 0.05  ~ "p<0.05",
    p < 0.1  ~ "p<0.1",
    .default = sprintf("p=%.2f", p)
  )
}

# Quantiles shown in boxplots
bp_prob <- c(0.1, 0.25, 0.5, 0.75, 0.9)
@

<<echo=FALSE, message=FALSE>>=
disagreement_range <- function(df, column) {
  df |>
    summarize(
      min_i = min({{ column }}),
      max_i = max({{ column }}),
      .by = `_citekey`
    ) |>
    summarize(
      min = sum(min_i > 0),
      max = sum(max_i > 0)
    )
} 

ranges <- df_c |>
  mutate(
    n_bg_over_third = fraction_background > 33,
    n_complete_yes = is_complete,
    n_complete_no  = !is_complete,
    n_igap_no      = icount == 0,
    n_igap_yes     = icount >= 1,
    n_igap_yes3    = icount >= 3,
    n_announce_no  = announcecount == 0,
    n_announce_yes = announcecount >= 1,
    n_ugap_no      = ucount == 0,
    n_ugap_yes     = ucount >= 1,
    n_proper_yes   = is_proper,
    n_proper_no    = !is_proper,
  ) |>
  select(starts_with("n_") | `_citekey`) |>
  reframe(across(starts_with("n_"),
    ~disagreement_range(pick(everything()), .x),
    .names = "{.col}")) |>
  unnest_wider(everything(), names_sep = "_") |>
  pivot_longer(cols = everything(),
    names_to = c("name", ".value"),
    names_pattern = "(.+)_([^_]+)$")

stats_overview <- df_c |>
  summarize(
    n_read_good       = sum(fkscore > 30),
    n_read_acceptable = sum(between(fkscore, 20, 30)),
    n_read_improper   = sum(fkscore < 20),
    n_bg_over_third   = sum(fraction_background > 33),
    n_complete_yes    = sum(is_complete),
    n_complete_no     = sum(!is_complete),
    n_igap_no         = sum(icount == 0),
    n_igap_yes        = sum(icount >= 1),
    n_igap_yes3       = sum(icount >= 3),
    n_announce_no     = sum(announcecount == 0),
    n_announce_yes    = sum(announcecount >= 1),
    n_ugap_no         = sum(ucount == 0),
    n_ugap_yes        = sum(ucount >= 1),
    n_ambig_no        = sum(ignorediffs == 0),
    n_ambig_yes       = sum(ignorediffs >= 1),
    n_proper_yes      = sum(is_proper),
    n_proper_no       = sum(!is_proper),
  ) |>
  mutate(across(everything(), ~./2)) |>
  pivot_longer(everything()) |>
  left_join(ranges, by = join_by(name)) |>
  mutate(across(c(value, min, max), ~./(nrow(df_c) / 2),
    .names = "r_{.col}"))

r_stat <- function(metric) {
  stats_overview |>
    filter(name == metric) |>
    pull(r_value) |>
    format_percent()
}
r_min <- function(metric) {
  stats_overview |>
    filter(name == metric) |>
    pull(r_min) |>
    format_percent()
}
r_max <- function(metric) {
  stats_overview |>
    filter(name == metric) |>
    pull(r_max) |>
    format_percent()
}
r_range <- function(metric) {
  stats_overview |>
    filter(name == metric) |>
    select(r_min, r_max) |>
    mutate(across(everything(), format_percent)) |>
    glue_data("[{r_min}, {r_max}]")
}
@

<<>>=
# Basic counts and ratios for our abstract
(n_abstracts <- n_distinct(df$`_citekey`))
(r_complete  <- r_stat("n_complete_yes"))
(r_read_good <- r_stat("n_read_good"))
(r_proper    <- r_stat("n_proper_yes"))
@

\begin{abstract}  % 100 to 200 words are allowed (but not enforced)
\emph{Background:}
Abstracts are a very valuable element in a software engineering research article,
but not all abstracts are as informative as they could be.
\emph{Objective:}
Characterize the structure of abstracts in high-quality SE venues,
observe and quantify deficiencies,
and suggest guidelines for writing informative abstracts.
\emph{Methods:}
Use qualitative open coding to derive concepts that explain relevant properties of abstracts
and identify the archetypical structure of abstracts.
Use quantitative content analysis to objectively characterize abstract structure
of a sample of \Sexpr{n_abstracts} abstracts from five presumably high-quality venues.
Use exploratory data analysis to find recurring issues in abstracts.
Compare the archetypical structure to actual structures to derive
guidelines for producing informative abstracts.
\emph{Results:}
Only \Sexpr{r_complete} of the sampled abstracts are \textit{complete},
i.e., provide background, objective, method, result, and conclusion information.
For structured abstracts, the ratio is twice as big.
Only \Sexpr{r_proper} of the abstracts are \textit{proper},
i.e., they also have good readability (Flesch-Kincaid score) and have
neither informativeness gaps nor understandability gaps or highly ambiguous sentences.
\emph{Conclusions:}
(1)~Even in top venues, a large majority of abstracts are far from ideal.
(2)~Structured abstracts tend to be better than unstructured ones,
but (3)~artifact-centric works need a different structured format.
(4)~The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.
\end{abstract}

\begin{IEEEkeywords}
!!!.
\end{IEEEkeywords}


%========================================================================
\section{Introduction}
\IEEEPARstart{A}{lthough} the abstract is a super important part of any research article~\cite{Lang22},
when reading an abstract in software engineering -- even in a presumably top-quality venue --
we often feel it is lacking important information or find it difficult to understand at all.

We aim to substantiate this impression
by operationalizing \emph{abstracts quality} and analyzing hundreds of abstracts,
and formulate constructive advice for writing better abstracts.


%Ideally, an abstract should
%situate the work within software engineering,
%motivate and state the issue addressed by it,
%describe the methods applied,
%report empirical results,
%and formulate a useful take-home message.
%With the possible exception of discussing related work,
%this is much the same agenda as for the article itself,
%just with much less detail;
%getting it right is no easy task.


\subsection{Research Questions}

% we use the numbers only in this subsection and the next;
% everywhere else we repeat the question or paraphrase it.
\noindent
We ask several questions:
\begin{enumerate}[label=\textbf{RQ\arabic*}, leftmargin=12pt, itemindent=15pt]
	\item What does a typical well-written abstract look like?
	\item \textbf{a)} Which deficiencies occur? \textbf{b)} How often?
	\item Do structured abstracts have better quality than unstructured ones?
	\item How \emph{should} software engineering abstracts be written?
\end{enumerate}
Of these, RQ1 and RQ2 should be considered exploratory,
RQ3 is hypothesis-driven (we expect a `yes'), and
the answer to RQ4 will be derived from the answers to the other three.


\subsection{Research Approach}

No firm expectations regarding RQ1 and RQ2a exist for engineering articles,
so qualitative methods will have to be used for them:
We start our research with \emph{open coding} of software engineering abstracts
to derive a vocabulary (a set of concepts or \emph{codes}; \cite[Ch. 5]{StrCor90})
by which the nature of a particular abstract can be characterized.

We intend to convince even readers who are skeptical of qualitative research
regarding our answers to the research questions and the need to improve
the quality of software engineering abstracts.
We therefore perform a repeatable
quantitative content analysis~\cite[Ch. 7]{Krippendorff04}
on a large sample of \Sexpr{n_abstracts} presumably high-quality abstracts
and apply an elaborate eight-step approach for maximizing its reliability.

The final stage is a statistical evaluation of the content analysis data,
which is again exploratory and straightforward in both
the questions it asks and the statistical methods it applies.


\subsection{Research Contributions}

Our contributions correspond to the research questions as follows:
\begin{enumerate}[label=\textbf{RQ\arabic*}, leftmargin=12pt, itemindent=15pt]
	\item As for the structure of well-written abstracts, we present an ``abstracts archetype''
	  that describes fixed parts of the structure and degrees of freedom
	  (Section~\ref{archetype}).
	\item We describe and discuss nine types of deficiencies,
	  and quantify the frequency of each deficiency type for the entire sample of abstracts
	  as well as for different subgroups of interest
    (Section~\ref{results} and Table~\ref{tab:overview}).
	  % Deficiency Types:
	  %  1. Readability (Sec 4.1, result-len-read)
	  %  2. Inefficient Allocation of Space (Sec 4.4, inefficient_allocation)
	  %  3. Murky OBJECTIVE Statements (Sec 4.5, murkyobjectives)
	  %  4. Missing Elements (Sec 4.6, missing_elements)
    %  5. Convoluted Trains of Thought (Sec. 4.7, convoluted)
    %  6. Informative Gaps (Sec. 4.8.1, igaps)
    %  7. Announcements (Sec. 4.8.2, announcements)
    %  8. Understandability (Sec. 4.9, ugaps)
    %  9. Ambiguity (Sec. 4.10, ambiguity)
	\item We present convincing data that structured abstracts tend to be better
	  in several respects
	  (Sections~\ref{missing_elements} \& \ref{structuredgood}).
	\item We provide data-based how-to instructions for writing abstracts for authors
	  as well as guidance for editors and conference organizers
    (Section~\ref{howto-guidelines}).
	  Software Engineering works should use structured abstracts, but
	  need a different and more flexible template than what is used so far.
\end{enumerate}


%========================================================================
\section{Related Work}

There is considerable literature on research abstracts across disciplines.
We will not attempt to summarize it here,
but provide examples of the different major perspectives of those studies
and otherwise focus on what has been done in the software engineering domain.


\subsection{Abstracts Structure}

Swales~\cite{Swales90} introduced \emph{genre analysis} as a means for teaching academic
reading and writing especially to non-native speakers:
Genres are ``classes of communicative events'' (e.g., \emph{the writing and reading of abstracts})
that are owned by a ``discourse community'' (e.g., \emph{software engineering researchers});
genre analysis means deconstructing texts (from a genre) to better understand
their elements in
terms of their syntactical structure, content, role, and interrelationships
(e.g., their relative position within the whole).

For our purposes here, the most relevant idea from genre analysis is the notion
of ``moves'', which are, roughly speaking, the building blocks used by writers
for making their overall point.
Several studies have looked at the move structure of research abstracts
in different fields such as
applied linguistics~\cite{DosSantos96} or
protozoology~\cite{CroOpp06}.
Despite the differences of research content, they find very similar moves,
such as this five-move structure~\cite{CroOpp06}:
\begin{quote}
	(1)~situate the research within the scientific community;
	(2)~introduce the research by describing the main features or presenting its purpose;
	(3)~describe the methodology;
	(4)~state the results;
	(5)~draw conclusions or suggests practical applications.
\end{quote}
This reflects, in slightly extended form, the IMRAD structure 
(Introduction, Methods, Results, and Discussion) of the body of scientific articles
that has gradually become the norm since the 1940s \cite{SolPer04}.

We found a similar structure for abstracts of 
empirical works in software engineering (see Section~\ref{archetype}),
but abstracts of artifact-centric works (tool building) do not fit this model and need
an extended one \Todo{point to a section}.


\subsection{Abstracts Quality}\label{relatedworkquality}

Several (meta-)studies on abstracts focus on quality assessment,
most often in subfields of the biomedical domain.
Many such studies cover articles of a homogeneous nature:
all randomized controlled trials (controlled experiments).
This allows formulating very specific expectations regarding what information should be
presented in an abstract and allows performing the analysis in checklist fashion,
for example:
In clinical dermatology, \cite{DupKhoLeb03} used a 30-item checklist on 197 abstracts
  for computing a 0-to-1 score and found mean scores between 0.64 and 0.78 for their various subgroups.
In dental medicine, \cite{ShaHar06} used a 29-item checklist on 100 abstracts
  and found a mean score of only 0.54.
  % suggests an 8-move structure for structured abstracts:
  % objective, design, setting, patients, interventions, outcome measure, results, conclusion.
  % per-move deficit rates are not reported.
Among 303 abstracts of cost-effectiveness analyses,
  29\% did not report the baseline to which the intervention
  had been compared~\cite{RosGreSto05}.
Among 146 abstracts of meta-analyses in peridontology, 
  33\% did not even report the direction in which  
  % it is indeed 32.9% percent: (146-15-83)/146, Table 2
  the evidence was pointing~\cite{FagLiuHud14}.
  
Various studies have investigated ``spin'' in the context of significance testing.
The term covers two types of behavior: 
Using language that sounds more positive than warranted or
reporting a secondary or alternative statistic as if it was the main one of interest.
Among all abstracts reporting non-significant results, studies found spin in 
  45\% of abstracts of orthopedic controlled experiments~\cite{ArtZaaChe20},
  44\% in emergency medicine~\cite{ReyRidBro20},
  56\% in psychiatry and psychology~\cite{JelRobBow20},
  and 58\% for the conclusions alone across a broad set of 
    medical controlled experiments~\cite{BouDutRav10}.

Unfortunately, the methods of those meta-studies are not applicable to a 
broad sample of software engineering abstracts, because
in fields with heterogeneous study structures such as ours, the operationalization of
\emph{quality} is less straight-forward.
For example, spin can take many more forms in software engineering articles
than is assumed by the studies mentioned above and it is difficult
to decide which forms are acceptable and which are not.
\Todo{Sounds ominous! Examples?}

One approach for discussing the quality of a software engineering abstract 
could be through comparing to a known ``good'' structure for abstracts.
For instance~\cite{CroOpp06} remarks
that one third of the 12 analyzed abstracts is lacking move 2 (stating a purpose).


\subsection{Structured vs.\ Unstructured Abstracts}\label{relatedworkstructured}

Many studies of abstracts quality do not study quality in general.
For instance, neither~\cite{DupKhoLeb03} nor~\cite{ShaHar06} reports
which of their checklist items are missing most frequently.
Rather, their research question is the relative quality of structured
versus unstructured abstracts.
A \emph{structured abstract} is one that uses a prescribed sequence of 
intermediate headings, such as Background, Objective, Methods, Results,
Conclusions or some similar set.
In almost all of those meta-studies, including~\cite{DupKhoLeb03} and~\cite{ShaHar06},
the answer is: structured abstracts have fewer quality issues.
Such research can be highly influential.
For instance, the CONSORTS report~\cite{MohHopSch12}
(containing guidelines for reporting controlled experiments, with over 10,000 citations),
relies on such a study \cite{HarSydBlu96} to recommend structured abstracts.
% https://journals.sagepub.com/doi/epdf/10.1177/016555159602200503

In software engineering, Kitchenham proposed
\emph{Evidence Based Software Engineering} (EBSE) in 2004 \cite{KitDybJor04}.
EBSE relies a lot on Systematic Literature Reviews (SLR).
The practicality of SLRs hinges on the informativeness of abstracts:
Can the researcher decide quickly and reliably, whether the present article
belongs in the SLR or not?

Therefore, Kitchenham performed two studies on structured abstracts
in software engineering.
The first took 23 published non-structured abstracts, 
converted them into structured ones,
and compared the two versions. 
It found that the structured abstracts were much longer,
but also had much better readability scores \cite{KitBreOwe08}.
The second, by Budgen, Kitchenham, and others \cite{BudKitCha08},
is a controlled experiment based on similar pairs of abstracts
rewritten into the structure 
Background, Aim, Methods, Results, Conclusions.
20 students and 44 researchers and practitioners
each judge one structured and one different unstructured abstract
for completeness (using an 18-item checklist) and 
clarity (using a vague 1-to-10 scale).
The structured format was found to increase the 
completeness score by 6.6 and the clarity score by 3.0.
70\% of the subjects also preferred the structured format
subjectively.
Both studies use only abstracts of purely empirical studies,
not tool-building works, 
which have very different (and more complicated) properties
as we will see \Todo{Pointer?}.



%========================================================================
\section{Methods}


\subsection{Overview}

Our study is a full-blown content analysis in the sense of Krippendorff~\cite{Krippendorff04}:
not just a counting exercise with a fixed codebook,
but rather an iterative codebook development before (and during) the counting
and an extensive abductive inference exercise after the counting.

It can be conceptualized as consisting of four widely overlapping stages or phases
as shown in Figure~\ref{qabstracts_timeline_commits}:
%
\Plot{qabstracts_timeline_commits}{%
	Timeline of the main study phases and their individual events.
    Each character's x-coordinate represents the time of a git commit.
    The vertical scattering is added for legibility only.}
%
Codebook development, which started first and is described in Sections~\ref{meth_codingrules}
and \ref{meth_codebook},
defined the rules and target concepts of the counting.
Training was for developing a joint understanding of the codebook and also contributed
greatly to the codebook's early evolution;
it is described in Section~\ref{meth_training}
Coding worked on a large sample of abstracts from top-quality venues described in
Section~\ref{meth_sample} and produced the count data subsequently used in the statistical analysis.
Coding is described in Section~\ref{meth_coding}.
\Todo{For Julian: Describe the Statistical evaluation}

Overall, we consider our study to be a qualitative one, but note that the middle two elements,
coding and statistical evaluation,
are compatible with a positivist epistemology (aiming for ``objective'' results) so that
the final interpretation is done on a solid quantitative foundation.


\subsection{Data Availability}\label{dataavailability}

We publish not only the outcome of our study, but also most parts of its development and
execution history in full detail: as a git version repository.
It includes all versions of the
codebook, handling procedure, coded abstracts, Python scripts for automation,
Python scripts for tabulations and plots, and the manuscript of this article.
Find it at \url{https://github.com/serqco/qabstracts/}.
\Todo{For Lloyd (after acceptance): Make the repository public and create a Zenodo repository link to a ``release'' of the GitHub repository.}

When we refer to abstracts from our sample, we use abbreviations of the first three authors' names and the year of publication, e.g. \Art{BesMarBos22}.
Refer to the repository in order to find the corresponding bibliographic information
and the complete abstract text for the respective article.


\subsection{General Coding Rules}\label{meth_codingrules}

Besides the definition of the content categories (codes),
our codebook contains global rules for the coding that can be summarized as follows:
\begin{itemize}
\item In order to limit complexity and avoid arbitrariness, we code by sentence and
  prefer single codes per sentence over multi-codings.
\item In order to mimic ordinary readers, when choosing a code, 
  we consider only what we have seen before plus
  one sentence forward context when needed (and only when needed).
\item In order to avoid excessive criticism of abstracts quality,
  we avoid coding negative properties whenever 
  an alterative, more positive interpretation is plausible as well.
  \Todo{Example? Not as clear as rule 1 and 2.}
\end{itemize}


\subsection{Codebook and Codebook Development}\label{meth_codebook}


\subsubsection{\Cb{background, objective, method, result, conclusion}}\label{meth_basicparts}

The codebook was initialized with concepts for the five sections commonly used
in a structured abstract:\footnote{In Krippendorff's terminology, this is an ``established theories'' justification
of analytical constructs~\cite[Section 9.2.3]{Krippendorff04}.}
\Cb{background, objective, method, result, conclusion}.
These represent, respectively: context information, the study goal or question,
empirical approach, empirical outcomes, and a take-home message that generalizes beyond the results.

Each code is defined by a short verbal explanation.
For example the definition of \Cb{method} reads
\QuoteCB{information about the approach or setup of an empirical (or possibly purely mathematical) study.}.
The initial definitions were made more and more precise and unambiguous by later
codebook refinements.
For example for \Cb{method}, the part \QuoteCB{(or possibly purely mathematical)} was initially not present
and was added when we encountered the first of those (very few) purely mathematical studies
during the coding process and were confused which code was appropriate for some sentence.
To help disambiguation, a few of the definitions need to be much longer than the above.


\subsubsection{Artifact-centric Studies: \Cb{design}}\label{design}

In a phase internally called ``prestudy'', the codebook was then refined and extended
based on coding attempts for a stratified sample of 20 abstracts from ICSE 2021.
% we used every 7th article, which constitutes stratification by virtue of the
% topic-centric session blocks in which the program is arranged.
We quickly recognized that additional codes were
needed.\footnote{In Krippendorff's terminology, this is an ``expert knowledge and experience'' justification
    of analytical constructs~\cite[Section 9.2.2]{Krippendorff04}:
    Being software engineering researchers ourselves, 
    we recognize when a sentence makes a different kind
    of contribution to an abstract than can be described by existing codes,
    and we are able to define what kind of contribution it is.}
Most importantly,
many software engineering articles do not talk about only an empirical study.
Rather, their focus is the design of some artifact, most often a tool,
sometimes a method or something else.
Much of the abstract is then spent on design considerations, design decisions,
techniques applied in implementation, and so on.
Such artifact-centric studies, although they also usually contain an empirical study,
are quite different in nature from purely empirical studies;
we therefore introduced the code \Cb{design} to mark such material.\footnote{\Cb{design} has
  the most detailed definition of all our codes: 170 words.}

We call the articles that contain at least one \Cb{design} coding in their abstract
``design works'', the others ``empirical works''.


\subsubsection{Refinements: \Cb{gap, summary, fposs} etc.}

At various later points during our study, we recognized a need for more granular coding
in order to capture differences in abstract writing we wanted to measure.
This led to the splitting of existing codes and the introduction of additional ones.
We then reworked existing codings to use the new codes consistently throughout.

The most important cases of such additional codes are these:
\Cb{gap} sentences state what is unknown or not yet possible \Todo{\Cb{need}?};
\Cb{summary} sentences summarize several results, but do not provide new information.
In contrast to a \Cb{conclusion}, a summary statement does not generalize beyond the immediate results.
\Cb{fposs}, \Cb{fneed}, \Cb{fwork} sentences occur at the end of abstracts.
They state what future work is now possible, needed, or planned-by-the-authors.
% I don't think examples are needed here. These are very understandable codes, and they are explained well here.


\subsubsection{Codes for Announcements: \Cb{a-*}}

Sometimes, statements insinuate there will be certain information 
in the article body, but do not provide any concrete information themselves.
We call such statements announcements and provide extra codes for them.
For example, here is an \Cb{a-method} (method announcement) from \Art{BesMarBos22}:
\Quote{As a second step, this study sets out to specifically
  provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based
  on an encouraging mindset and attitude from both managers and technical roles to understand how, when and by
  whom such strategies are adopted in practice.}
Despite the long sentence, we learn nothing about how the assessment
or the analysis work.
Here is an \Cb{a-result} (results announcement) from \Art{FlyChaDye22}:
\Quote{Based on those results, we then used the Boa and Software
  Heritage infrastructures to help identify and quantify several sources of dirty Git timestamp data.}
The first part is \Cb{method}, the second should have been \Cb{result},
but, alas, we learn nothing about those sources' nature or number or impact.


\subsubsection{Codes for Headings: \Cb{h-*}}

We also have codes for the headings (only the headings words themselves) used in structured abstracts.
These codes are conceptual, i.e., for instance
\Quote{Aim:}, \Quote{Goal:}, \Quote{Objective:}, \Quote{Question:}, and their plural forms
would all be coded as \Cb{h-objective}.
Likewise, there are \Cb{h-background}, \Cb{h-method}, and so on.
We recognize structured abstracts by the presence of such a heading code.


\subsubsection{Subjective Additions: \Cb{:i, :u}}

The codes described so far aim at codifying repeatable properties,
where several well-trained coders will come to the same result with high probability.
In addition, we defined a number of suffixes for codes, by which coders can provide
additional information for which the expectation of agreement is much lower.
These are also not neutral, like the codes themselves, but all describe some
kind of deficiency.
The most important of these suffixes are the following:

Informativeness gaps (coded as \Cb{:i})
are spots in a sentence where the coder desired to know
additional detail that is presumably available to the authors and
that can presumably be provided in very little space.
Example (from \Art{LiuFenYin22}):
\Quote{To evaluate DeepState, we conduct an extensive empirical study on popular datasets
and prevalent RNN models containing image and text processing tasks.}
This sentence was coded as \Cb{method:i2}, because the coder asked themselves
\Pseudoquote{How many datasets? How many RNN models?}
The answers are both given in that article's Table 3: Four datasets, three models.
The authors could and should have given that information in the abstract.

Understandability gaps (coded as \Cb{:u})
are spots in a sentence where the coder finds their usual intuitive
half-understanding of a term used in the abstract insufficient for understanding
the abstract overall.
Example (from \Art{CheHuWei22}):
\Quote{Finally, we propose a new dynamic vocabulary strategy which can effectively resolve the UNK problems in code summaries.}
This sentence was coded as \Cb{design:u1} \Todo{Why not \Cb{:u2}?}
because it is unclear from the abstract what the unintroduced concept of \Quote{UNK problems} are,
and furthermore, what that means for \Quote{code summaries}.
% Here are all of the other candidate "easy-to-understand examples of :u1" that I found:
% The data were analyzed using a Bayesian independent sample t-test and **network
% analysis**.
% {{method:u1}}
% The evaluation results suggest that their edge coverage performance can be **unstable**.
% {{result:u1}}
% Our results motivate short-term patches and **long-term fundamental solutions**.
% {{conclusion:u1}}
% In this paper, we propose **CDCS**, a novel approach for domain-specific code search.
% {{objective:u1}}


\subsection{Training}\label{meth_training}

The training phase (internally called ``prestudy2'') served two purposes:
Finding/repairing deficiencies in the codebook,
and arriving at a joint interpretation of it across the four coders
(the four authors\footnote{Four additional people were involved in the training phase at some
  point, but they decided not to join the full study.})
As you can see in Figure~\ref{qabstracts_timeline_commits},
the training phase extended over almost half a year and triggered
the majority of the codebook improvements.

In the training phase, we perfected the mechanics of the coding process
described below, in particular the very useful \Prg{compare-codings} script
and email routine,
and generally formed as a research team.
\Todo{Obscure}


\subsection{Sample}\label{meth_sample}

We decided not to aim for a broad selection of all software engineering research,
but rather concentrate on what is presumably the highest quality material:
The ICSE technical research track,
the three journals allowed for journal-first presentations at ICSE
(Empirical Software Engineering EMSE,
ACM Transactions on Software Engineering and Methodology TOSEM,
IEEE Transactions on Software Engineering TSE).
Since we expected to find that structured abstracts had better quality
than unstructured ones, we added a fifth venue that required structured abstracts:
Information and Software Technology (IST, an Elsevier journal).
IST has published many very good systematic literature reviews and methods works,
but is not \emph{generally} considered a top-quality venue.

<<>>=
# Abstracts per venue
(n_abstracts_per_venue <- count(df, venue) |>
  glue_data("{venue}:{n}"))
@

We wanted to draw a random sample of 100 articles per venue from the
2022 volumes, but found that TOSEM has published only 86 articles that year,
so we ended up at 486 articles initially.
A few of those later had to be removed because they were other things,
often editorials.
Furthermore, we eventually did not need quite as much data for answering our
questions and stopped coding after \Sexpr{n_abstracts} abstracts\footnote{After abstracts 
	were dropped, our sample is no longer perfectly balanced, with \Sexpr{n_abstracts_per_venue} abstracts.}.
Still, ours is the largest manual study of abstracts we know of.

Volume downloading, sampling, and abstract extraction into publishable
and annotation-ready text files were all done automatically by the scripts
\Prg{retrievelit}\footnote{\url{https://github.com/serqco/retrievelit/}},
\Prg{select-sample}, and \Prg{prepare-sample} --- except
that the EMSE article format required manual abstracts cleansing.
All these tools were purpose-built for the present study.


\subsection{Coding Process}\label{meth_coding}

We code each abstract twice, by so-called coders A~and~B.
Abstracts are held in text files in separate directories \Prg{abstracts.A} and \Prg{abstracts.B}.
Each sentence is followed by a line containing a pair of curly braces \Prg{\{\{\}\}}
into which the coder would enter their codings,
such as \Prg{\{\{method,result:i2\}\}} for a complex sentence that contains substantial
amounts of method information as well as results with two informativeness gaps.
We batch the coding in blocks of 8 abstracts each.
Coders pick and process blocks based on their available time, resulting in different numbers of blocks
done by each author, between 13 blocks for Franz and 31 blocks for Lutz. 
The procedure, coordinated via git, is best explained by example,
which we do in the following two subsections.


\subsubsection{Coding}

\textbf{Step~1.} When Lloyd wanted to code a block of abstracts on 2023-05-26,
he found the next available block to be Block~17, which was already coded once by Lutz (``coder~A'').
Lloyd reserved his spot as ``coder~B'' in the coordination file \Prg{sample-who-what.txt}
and performed the coding.
\textbf{Step~2.} He then ran \Prg{check-codings} to test his codings
against the codebook and corrected any mistakes, such as typos.
\textbf{Step~3.} He then ran \Prg{compare-codings} to compare his codings against Lutz'.
This script creates one \emph{report block} for each sentence where the codings
of coders A and B are not compatible.
Compatible means: Differing at most in the subjective suffixes \Cb{:i} and \Cb{:u}, but not in the codes
(and not by more than one in the numbers of informativeness gaps and understandability gaps).
If Lloyd found a report block where Lutz' coding was obviously correct and his own
obviously wrong, he would simply correct his coding.
\textbf{Step~4.} He would then commit his coded abstracts into git.

\begin{lstlisting}[language={},
  caption={Example of a coded abstract: \Art{RosClaMad22}.},
  captionpos=t,
  label={lst:coding_example},
  breaklines=true,
  breakindent=0pt,
  moredelim={[s][keywordstyle]{\{\{}{\}\}}},  % Adds bold styling to our codes. Added for visual clarity
  basicstyle=\scriptsize]
Empirical Effort and Schedule Estimation Models for Agile Processes in the US DoD.

Estimating the cost and schedule of agile software projects is critical at an early phase to establish baseline budgets and schedules for the selection of competitive bidders.
{{background}}
The challenge is that common agile sizing measures such as story points and user stories are not practical for early estimation as these are often reported after contract award in DoD.
{{gap}}
This study provides a set of effort and schedule estimation models for agile projects using a sizing measure that is available before proposal evaluation based on data from 36 DoD agile projects.
{{objective,method}}
The results suggest that initial software requirements, defined as the sum of functions and external interfaces, is an effective sizing measure for early estimation of effort and schedule of agile projects.
{{conclusion}}
The models' accuracy improves when application domain groups and peak staff are added as inputs.
{{conclusion}}
---
\end{lstlisting}

\subsubsection{Handling Disagreements}


\textbf{Step~5.} For the remaining (unresolved) report blocks, Lloyd would write an email to Lutz explaining his reasoning.
\textbf{Step~6.} Lutz would read through that email and categorize the report blocks into the following cases:
\begin{enumerate}[label=\alph*)]
  \item Lloyd's coding is obviously correct
  (and Lutz' own is a clerical error, as in Figure~\ref{email-MeyAlmKel22.png}) or
  Lutz prefers Lloyds coding.
  Lutz adjusts his coding accordingly.
  \item Lutz finds Lloyd's coding clearly incorrect (clerical error) or
  Lutz prefers his own coding.
  He would respond with an explanation of his reasoning and suggest that
  Lloyd either adjust his coding or add an \Cb{-ignorediff} marker to it.
  This suffix indicates two accepted alternative interpretations of the same sentence.%
  \footnote{It silences the \Prg{compare-codings} script for this particular report block, so that the coding difference is now officially accepted.}
  \item Lutz finds both codings equally acceptable.
  He would add an \Cb{-ignorediff} marker to his own and respond accordingly, explaining his reasoning.
\end{enumerate}
\textbf{Step~7.} Lutz would commit his corrected abstracts and send the response email.
\textbf{Step~8.} Lloyd would read the response email and usually act on it to finish the handling of
Block~17.
Only very rarely would he disagree with something to a degree that would make
another round of emails necessary.

\begin{figure*}[tbp]%
	\centering\fbox{\includegraphics[width=0.95\textwidth]{email-MeyAlmKel22.png}}%
	\vspace{-2mm}\caption{Excerpt from Lutz' response email during the disagreements handling
		for Block 17.
		Lines 1--7: Report block generated by the \Prg{compare-codings} script (\textbf{Step 3});
    line 9: text line from Lloyds first email (\textbf{Step 5});
    line 11: Lutz' response (\textbf{Step 6}).
		This one was a simple case;
		sometimes each coder provided several sentences of argumentation.
		Overall, Lloyd's first email contained report blocks for 9 disagreements
		in 4 abstracts, both typical numbers.}\label{email-MeyAlmKel22.png}%
\end{figure*}


\subsubsection{Effect of this Procedure}

Most content analyzes code most of their material only once,
then code a random subsample a second time,
compute some coefficient of agreement,
report it as a measure of good-enough coding quality,
and that's that.
In contrast, our above-described procedure has two effects:
\begin{enumerate}
	\item It maximizes the quality of the coding.
	  Very few clerical errors, if any, will have managed to escape our discussion process.
	  Besides, the definitions of all codes that are sometimes difficult to tell apart
	  have been refined until they were very mature.
	\item It finds all those spots in the sample where the abstracts are so convoluted
	  or strangely formulated that even our careful code definitions do not lead
	  to a canonical judgment.
	  These spots, which will be marked by a \Cb{-ignorediff} annotation in our data,
	  should clearly be considered to be badly written --- which is great,
	  because that is what our study is interested in.
\end{enumerate}
Note that, technically speaking, our procedure also leads to a 100\% perfect inter-coder agreement,
because even the cases of \Cb{-ignorediff} indicate that
the coders agree on multiple plausible interpretations of one sentence.


\subsection{Statistical Evaluation}\label{statisticalevaluation}

The difficult part of our statistical evaluation is asking the right questions;
our data provides a lot of possibilities.
In contrast, the actual statistical techniques are simple and straightforward:
Mostly tabulations of counts or percentages, bar plots, and box plots.
We define two binary properties of abstracts: 
A \emph{complete} abstract contains all of the basic elements described in 
Section~\ref{meth_basicparts}.
A \emph{proper} abstract is complete and does not have any of the deficiencies
described in the Results section as making an abstract \emph{improper}.

Since we coded each abstract twice, we get potentially conflicting assessments regarding these binary properties.
We calculate and report percentages across all \emph{codings}
($n_C\,{=}\,\Sexpr{nrow(df_c)}$, which each count as a ``half'' abstract),
as well as liberal and conservative percentages per \emph{abstract}
($n_A\,{=}\,\Sexpr{nrow(df)}$),
for which either both or just one coder is necessary to attest a deficiency.
In the text, we report these numbers in square brackets;
in the plots, we show them as error bars.
When we write
\Pseudoquote{\Sexpr{r_complete} \Sexpr{r_range("n_complete_yes")} of the abstracts are complete},
we mean that \Sexpr{r_complete} of the \Sexpr{nrow(df_c)} abstract codings qualify
the corresponding abstract as \emph{complete},
and that for \Sexpr{r_min("n_complete_yes")} of the \Sexpr{nrow(df)} abstracts both coders agreed,
while at least one coder thought so for \Sexpr{r_max("n_complete_yes")} of them.

We mostly refrain from performing significance tests or computing confidence intervals,
because most analyses are exploratory, not driven by specific expectations or theories.
The one exception from this rule is the comparison of
structured versus unstructured abstracts.

Most phenomena we report are gradual by nature.
In this spirit, we often use the following verbal terms for frequencies:
rare (less than 5\%),
not rare (5-20\%),
common (20-35\%),
frequent (35-50\%),
dominant (over 50\%).\Todo{For Lutz: Find spots where these should be used but are not.}


\subsection{Interpretation}

Our interpretation of the measurements is driven by our research interest and
guided by our expertise as software engineering researchers who read abstracts.
Whenever we mark a phenomenon as a weakness, we will provide a justification from that angle
and provide an example to allow the reader to relate to the justification.
\Todo{For Lutz: Find spots where such an example is missing.}


\subsection{Readability Metric}

For evaluating general readability at a purely language level,
we use the Flesch-Kincaid ``reading ease'' readability score \cite{KinFisRog75}.
It is a validated and widely used metric that judges readability of English text
based only on the number of words per sentence and the number of
syllables per word.
Values under 30 represent graduate-level difficulty,
under 10 extreme difficulty for native speakers.
Considering that most community members are not native speakers of English,
we judge 20 to 30 to be a range suitable for researcher audiences and so
we call anything over 30 ``good'',
20 to 30 ``normal'', and
under 20 ``improper''.


\subsection{Use of Examples}

We will use examples from real abstracts from our sample to illustrate
some of our statements.
We identify their source by a citation key
formed from three letters each of the first three authors' names.
For example article 1 in Block 1 in our study was written by
Fregnan, Petrulio, Di Geronimo, and Bacchelli
and is thus identified as \Art{FrePetGer22}.

We select those examples for the clarity of the phenomenon in question,
not for the abstract's quality.
For the source of every positive example there are others that are
as good or better.
For the source of every negative example there are others that are
as bad or worse.
Since the use of an example is not about the article it stems from,
we do not include those articles in our references list.

%========================================================================
\section{Results}
\label{results}

\subsection{Length and Readability}
\label{result-len-read}

<<>>=
# Typical lengths of abstracts
(typical_length_words <- df$words |>
  quantile(c(0.25, 0.75)) |>
  round(-1) |>
  paste(collapse = " to "))
(typical_length_sentences <- df$sentences |>
  quantile(c(0.25, 0.75)) |>
  paste(collapse = " to "))
@

<<>>=
# Are structured abstracts longer?
df |>
  reframe(qs = quantile(words, bp_prob), prob = bp_prob * 100,
    n = n(), .by = is_struc) |>
  pivot_wider(id_cols = c(is_struc, n), values_from = qs,
    names_from = prob, names_prefix = "p")
@

<<>>=
# Disobeying venues' word limits
df |>
  mutate(max_words =
    case_match(venue, c("EMSE", "TSE") ~ 250, "IST" ~ 300)) |>
  summarize(
    n = n(),
    n_over = sum(words > max_words),
    r_over = n_over / n,
    .by = venue)
@

Typical abstracts (the middle half) are \Sexpr{typical_length_words} words long
and \Sexpr{typical_length_sentences} sentences long.
Structured abstracts tend to be longer than unstructured ones.
\ifarxiv{See Figures \ref{boxplots_words} to \ref{boxplots_avg_wordlength} for details.}
The official word limits of
150--250 for EMSE,
maximum 300 for IST, and
recommended maximum 250 for TSE
are disobeyed by more than a quarter of all articles for each of these three venues.

<<>>=
# Readability scores
(r_read_normal   <- r_stat("n_read_acceptable"))
(r_read_improper <- r_stat("n_read_improper"))
(r_read_lt10     <- format_percent(sum(df$fkscore < 10) / nrow(df)))
@

<<>>=
# Readability by venue
(venue_mean_fkscores <- df |>
  summarize(n = n(), fk = mean(fkscore), fk_f = round(fk), .by = venue))
(venue_best_fk  <- venue_mean_fkscores |> slice_max(fk, with_ties = FALSE))
(venue_worst_fk <- venue_mean_fkscores |> slice_min(fk, with_ties = FALSE))
@

For readability, see Figure~\ref{boxplots_fkscore}:
only about \Sexpr{r_read_good} of all abstracts have good readability (>30),
about \Sexpr{r_read_normal} have normal readability (20-30),
and \Sexpr{r_read_improper} are improper (<20)---%
more than every fifth abstract (\Sexpr{r_read_lt10}) even has a score below 10!
\Sexpr{venue_best_fk$venue} tends to be better than the other venues,
\Sexpr{venue_worst_fk$venue} is worse.

<<>>=
py$annot <- import_from_path("qscript.annotations", path = "../script")

abstract <- readr::read_lines("our-abstract.txt") |>
  as_tibble() |>
  filter(!value %in% c("Background:", "Objective:", "Methods:", "Results:", "Conclusions:"))

syllablecount <- function(word) {
  py_eval(paste0("annot.AnnotatedSentence.syllablecount(\"", word, "\")"))
}

results <- abstract |>
  rename(sentence = value) |>
  mutate(
    word_list      = str_split(sentence, "\\s+"),
    word_count     = purrr::map_int(word_list, length),
    syllable_count = purrr::map_int(word_list, ~sum(purrr::map_int(.x, syllablecount)))
  )

total_sentences <- nrow(abstract)
total_words <- sum(results$word_count)
total_syllables <- sum(results$syllable_count)
our_fk_score <- 206.835 - 1.015 * total_words / total_sentences - 84.6 * total_syllables / total_words
@

\Plot{boxplots_fkscore}{%
	Flesch-Kincaid `reading ease' readability score, higher is better.
    Values under 50 are considered difficult to read (college-level material);
    under 30: very difficult to read (graduate level, acceptable for abstracts);
	  under 10: extremely difficult to read (overly difficult).
    Differences between subgroups are modest.
    \Sexpr{venue_best_fk$venue} is best (mean \Sexpr{venue_best_fk$fk_f}),
    \Sexpr{venue_worst_fk$venue} is worst (mean \Sexpr{venue_worst_fk$fk_f}).
    The abstract of the present article has a score of~\Sexpr{round(our_fk_score, 1)}.
    Structured abstracts are just as difficult as nonstructured ones
    if one ignores the ``Methods:'' etc. headings as we did for the readability analysis.
}


\subsection{Design Articles vs. Empirical Articles}

We described the idea of the \Cb{design} code in Section~\ref{design}.
But many empirical works involve some artifact design as well,
so how is the discrimination made?
That depends on how the authors phrase their goal in their
\Cb{objective} statement.
Consider the following goal statements:
\Quote{In this paper, we propose AI-based automation for the
completeness checking of privacy policies.} (from \Art{AmaAbuTor22}).
\Quote{This study aims to investigate the urgency and importance of
reproducibility and replicability for DL studies on SE tasks.} (from \Art{LiuGaoXia22}).

The first puts the artifact at the center, so this is a design work.
The second puts empirical results at the center, so this is an empirical work
and the \Cb{design} code will not be used.
Artifact design discussion will then usually be coded as \Cb{method} instead.
Mixed cases are rare and then the coders will decide which aspect has more weight.

<<>>=
# Useful pivot table
df_frac <- df_c |>
  pivot_longer(
    cols = starts_with("fraction_"),
    names_to = "part",
    values_to = "value",
    names_transform = list(part = ~str_remove(., "fraction_"))
  )

# Typical fractions for design works and empirical works
typl_frac <- df_frac |>
  summarize(
    n = n(),
    p25 = quantile(value, 0.25, na.rm = TRUE),
    p75 = quantile(value, 0.75, na.rm = TRUE),
    .by = c(is_design, part)
  )
typl_frac |> print(n = 2)

fmt_typl_frac <- function(design, part_name, glue = "--") {
  typl_frac |>
    filter(is_design == design, part == part_name) |>
    mutate(across(c(p25, p75), ~format_percent(./100))) |>
    glue_data("{p25}{glue}{p75}")
}
@

In design works, a large fraction of the abstract will be devoted to
describing design considerations for that artifact;
in our data: typically \Sexpr{fmt_typl_frac(TRUE, "design", " to ")} of the words.
The empirical study, which usually exists as well, then usually has a mere
supporting role (validating the claims made in the artifact discussion)
and is correspondingly given less space:
\Sexpr{fmt_typl_frac(TRUE, "method")}
(versus \Sexpr{fmt_typl_frac(FALSE, "method")} for empirical works) 
for description of empirical method,
\Sexpr{fmt_typl_frac(TRUE, "result")}
(versus \Sexpr{fmt_typl_frac(FALSE, "result")})
for description of empirical results.
Design work abstracts are barely longer than empirical work abstracts.

<<>>=
# Are design abstracts longer than empirical abstracts?
df_c |>
  reframe(qs = quantile(words, bp_prob), prob = bp_prob * 100,
    n = n(), .by = is_design) |>
  pivot_wider(id_cols = c(is_design, n), values_from = qs,
    names_from = prob, names_prefix = "p")
@


\subsection{The Abstracts Archetype}\label{archetype}

Compared to the content structure assumed by the usual formats of
structured abstracts, our codebook is more fine grained;
the \Cb{design} code is but one example.

During our sensemaking process, we had a number of insights regarding
how a well-written abstract ``ticks'', which we eventually distilled into
the following template, which we call the \emph{software engineering abstracts archetype}
(see also Figure~\ref{abstracts_archetype_fig}):

\begin{enumerate}
\item An abstract consists of three parts, in this order:
   \emph{Introduction}, \emph{Study Description}, and \emph{Outlook}.
\item Two turning points connect the three parts:\\
   a) A statement of the study goals (\Cb{objective}) connects \emph{Introduction}
      to \emph{Study Description}.\\
   b) A generalizing statement (``take-home message'', \Cb{conclusion})
     connects \emph{Study Description} to \emph{Outlook}.
\item The \emph{Introduction} first introduces the topic area of the study and what is known (\Cb{background})
   and then may or may not point out a gap in knowledge (\Cb{gap}).
\item For an empirical article, the \emph{Study Description} begins with
   method description (\Cb{method}), followed by results description (\Cb{result}).
   Sometimes, this sequence occurs twice in a row, very rarely more.
\item For a design article, design description (\Cb{design}, see below) precedes the structure
   described in the previous point.
% \item Occasionally (but infrequently), \emph{Study Description} will end with a study summary (\Cb{summary}).
\item After the \Cb{conclusion}, the \emph{Outlook} talks about future research and states
   what could now be done (\Cb{fposs}, for future possibilities),
   what should now be done (\Cb{fneed}),
   what the authors themselves intend to do (\Cb{fwork}), or
   what is still not known (\Cb{fgap}).
   Several statements of each type may occur, in no particular order.
\end{enumerate}

The archetype describes all variants of abstracts that have a natural train of thought,
deviations will tend to lead to a less easily understandable abstract.
\Todo{Provide IMRAD-based justification}

\Todo{Find short, good, unstructured example and mark it up. Lloyd: Perhaps not necessary now that we have the Archetype figure ...?}

\PlotWidth{abstracts_archetype_fig}{.8}{%
	A visual representation of the Abstracts Archetype. Note that this figure only describes the ``natural train of thought'' and not all possible abstract forms that we coded in this study. This is the recommended structure.}


\subsection{How Not To: Inefficient Allocation of Space}
\label{inefficient_allocation}

If one reads those abstracts, one can hardly help notice that some of them 
spend a lot of space on \Cb{background}, although its only purpose is to situate
the objective and make it understandable.
Here is an example from an article titled
``!!!'':
\begin{quote}
  \Todo{Does an example pay off here? It needs a lot of space}	
\end{quote}	

Other abstracts manage to solve the same problem in a wonderfully concise manner:
\begin{quote}
	\Todo{Find example}	
\end{quote}	

\Plotwide{box_xletgroups_topicfractions}{%
	Per-topic distribution of the amount of space used for that topic.
	These ``topics'' are groupings of related codes; e.g. Outlook stands for the union of
	\Cb{fposs}, \Cb{fneed}, \Cb{fgap}, \Cb{fwork}, 
	and all corresponding \Cb{a-*} and (theoretically) \Cb{h-*} codes.\\
	\Describeboxplots
	\Describegroups}

\Plot{box_xletgroups_conclusionfractions}{%
	Comparison of the amounts of remaining space devoted to the conclusion for abstracts
	with a rather short background section (lowest quarter) vs those
	with a long one (highest quarter).
	The latter conclusions are shorter even though the indicated percentage 
	pertains to the part of the abstract after the background only.\\
	\Describegroups}

<<>>=
# p75 of fraction_background per venue
df_c |>
  summarize(p75_bg = quantile(fraction_background, 0.75),
    .by = venue) |>
  arrange(desc(p75_bg))

# abstracts with more than 1/3 background, per venue
df_c |>
  summarize(gt33_bg = sum(fraction_background >= 33.3)
    / n() * 100, .by = venue) |>
  arrange(desc(gt33_bg))
@

As we can see in the Background group of boxplots in Figure~\ref{box_xletgroups_topicfractions},
some venues have a quarter of articles that spend one third or more of the abstract space
on background. This leads to deficiencies later on, as we can see in
Figure~\ref{box_xletgroups_conclusionfractions}:
The conclusion is the potentially most useful part of the abstract---the take-home message,
but with a long background section, it tends to become pronouncedly shorter.

<<>>=
# Background lengths structured vs. unstructured
df_c |>
  reframe(qs = quantile(fraction_background, bp_prob),
    prob = bp_prob * 100, n = n(), .by = is_struc) |>
  pivot_wider(id_cols = c(is_struc, n), values_from = qs,
    names_from = prob, names_prefix = "p")
@

Background lengths are more benign for structured abstracts.


\subsection{How Not To: Murky \Cb{objective} statements}\label{murkyobjectives}

The \Cb{objective} statement is the key means by with an abstract communicates
to the reader what the work is all about. 
Getting it wrong is therefore probably the single biggest mistake to make
for the abstract writer.

We saw a number of near-incomprehensible cases in our sample.
For instance
\Todo{horrible objectives statement example 1}
should better have said
\Todo{appropriate replacement for example 1}.
And
\Todo{horrible objectives statement example 2}
should better have said
\Todo{appropriate replacement for example 2}.

\Todo{Argue with lengthy email discussions.}

\Todo{Anything quantitative?}



\subsection{How Not To: Missing Elements}
\label{missing_elements}

Given the archetype, one way to approach an analysis of abstracts quality is to ask
how often key parts of an abstract are missing entirely.
This is shown in Figure~\ref{zerofractionbar_xletgroups_topicmissingfractions}.

\Plotwide{zerofractionbar_xletgroups_topicmissingfractions}{%
	How often is a topic not present at all in an abstract?\\
	\Describegroups}

<<>>=
# Missing elements
(missing_elements <- df_frac |>
  summarize(
    n = n(),
    n_missing = sum(value == 0),
    r_missing = n_missing / n,
    .by = c(part, is_struc)
  ) |>
  arrange(is_struc, desc(r_missing)) |>
  filter(part %in% c("background", "objective", "method",
    "result", "conclusion")))

(r_no_conclusion <- missing_elements |>
  summarize(
    n = sum(n),
    n_missing = sum(n_missing),
    r_missing = n_missing / n,
  .by = part) |>
  filter(part == "conclusion")) 
@

The often-missing \Cb{gap} and \emph{Outlook} parts \Cb{fposs}, \Cb{fneed}, etc. are clearly optional,
so it is not a problem that they are often not present.
\Cb{background} and \Cb{objective} are hardly ever missing.
\Todo{The scale introduced early does not have ``hardly'', only ``very rare (<5\%)''}

\Cb{method} and \Cb{result} are sometimes missing (and this is a problem),
but never in structured abstracts.

The shocking part of this analysis is \Cb{conclusion}, which ought to be present
as the key take-home message in any abstract, but is in fact missing in more than
half of all---yet again very rarely in structured abstracts\footnote{Note that
  the prompt of having to write something after a ``Conclusion:'' keyword
  alone cannot guarantee this:
  The statement put there can be (and sometimes is) something else,
  typically a \Cb{result} statement or \Cb{summary}.}.

<<>>=
r_complete
(r_complete_range <- r_range("n_complete_yes"))
@

Overall, only \Sexpr{r_complete} \Sexpr{r_complete_range} of all abstracts are \emph{complete}.


\subsection{How Not To: Convoluted Trains of Thought}
\label{convoluted}

Botching the \Cb{objective} statement or leaving out important parts from an abstract
are not the only ways for wrecking the abstract's value, however.
There are also abstracts where the train-of-thought is so convoluted
that they become difficult to read.

For quantifying this, we map each abstract to a short sequence of letters,
where each letter stands for a contiguous stretch of sentences in the abstract
that have the same content type; the letter designates that content type.
For instance, an abstract that follows a minimal incarnation of the Archetype
for an empirical article would be encoded as \strct{bomrc}, which stands for
the content types sequence \textit{background, objective, methods, results, conclusion}.

<<>>=
(n_struc <- df |>
  select(`_citekey`, is_design) |>
  distinct() |>
  left_join(datasets$ab_structures, by = c("_citekey" = "citekey")) |>
  summarize(n = n(), n_distinct = n_distinct(topicstructure),
    .by = is_design))
@

The full list of abstracts structures is too long to show it here.
It has \Sexpr{n_struc[n_struc$is_design == FALSE, "n_distinct"]} entries for empirical articles and
\Sexpr{n_struc[n_struc$is_design == TRUE, "n_distinct"]} entries for design articles.
Most of these have only one or two instances, though;
we restrict ourselves to the types that occur with higher frequency here.
Note that a frequency of e.g. 2.5 simply means that the two coders did not agree.
The results are shown in
Figure~\ref{ab_topicstructure_freqs_empir} for empirical articles and
Figure~\ref{ab_topicstructure_freqs_design} for design articles.
\Todo{leave out these plots to save space? Or even move entire(?) section to appendix?}

\Plot{ab_topicstructure_freqs_empir}{%
  The frequency of different trains-of-thought in the abstract for empirical articles.
  The label is a string of stretch-code characters:
  b-ackground, g-ap, o-bjective, d-esign, m-ethod, r-esult, s-ummary, c-onclusion, O-utlook.}
\Plot{ab_topicstructure_freqs_design}{%
  The frequency of different trains-of-thought in the abstract for design articles.
  Same characters as before, except that d can now in fact occur.}

% !!!CheXiaLo22 presents five studies


\subsubsection{Empirical Articles}

<<>>=
assert_exists <- function(x, what) {
  if (length(x) == 0) paste0("\\Todo{", what, " does not exist}")
  else x
}

strc_example <- function(structure) {
  datasets$ab_structures |>
    filter(topicstructure == structure) |>
    slice_max(citekey, with_ties = FALSE) |>
    pull(citekey) |>
    assert_exists(structure)
}
@

\Todo{Adapt to actual plot}
Let us start with the simpler empirical articles.
We see that the most sensible structures are also the most frequent (bars 1, 2, 3):
Archetypical with or without a \Cb{gap} statement, perhaps with an \emph{Outlook}.
But from bar 4 on, the abstracts are more or less pathological:
The first few are mostly missing a conclusion, later on (not shown in the figure)
follow all kinds of curious structures such as, to pick an admittedly extreme case,
\strct{bgomrcsrsc} (e.g., \Art{\Sexpr{strc_example("bgomrcsrsc")}}):
two conclusions, two summaries, summary after conclusion.

\subsubsection{Design Articles}

\Todo{Adapt to actual plot}
It naturally gets worse for the design articles with their more complicated structure:
Even the top two structures are missing a conclusion
and we find complex repetitive structures starting at bar 5.
Not all of these are automatically bad.
In particular, having two small empirical studies, resulting in a nice \strct{bgodmrmrc} structure
(e.g., \Art{\Sexpr{strc_example("bgodmrmrc")}}), can make a lot of sense for the reader.
But this has its limits:
An abstract with structure \strct{bodmrmrmrmr}
(e.g., \Art{\Sexpr{strc_example("bodmrmrmrmr")}})
is overloaded.
It can also easily get out of hand, as confirmed by the examples
\strct{bobdrmrmr} (e.g. \Art{\Sexpr{strc_example("bobdrmrmr")}}) and
\strct{bomdmrmrmrsmrc} (e.g. \Art{\Sexpr{strc_example("bomdmrmrmrsmrc")}}).
\Todo{Validate they are indeed broken.}

\subsubsection{Structured Abstracts}

<<>>=
ab_is_design <- distinct(select(df, `_citekey`, is_design)) |>
  left_join(datasets$ab_structures, by = c("_citekey" = "citekey"))

(n_distinct_empir_struct <- ab_is_design |>
  filter(!is_design, is_struc) |>
  summarize(count = n_distinct(topicstructure)))
(n_distinct_empir_all <- ab_is_design |>
  filter(!is_design) |>
  summarize(count = n_distinct(topicstructure)))
@

Today's conventions for structured abstracts may be a bit restrictive
(e.g. by not accommodating the useful \strct{mrmr} substructure),
but they do result in more orderly abstracts:
There are only \Sexpr{n_distinct_empir_struct} different abstract structures
among the \emph{structured} abstracts of empirical articles
versus \Sexpr{n_distinct_empir_all} for empirical articles overall.
\Todo{replace numbers by mean of same-sized samples}


\subsection{How Not To: Uninformative Formulations}

A milder way of reducing the usefulness of an abstract is using formulations that
fail to provide information that, at this point, would be useful to the reader and
could be provided using only very few additional words.

Our investigation has identified and then quantified two types
of such lacks of informativeness.
We call them informativeness gaps and announcements, respectively.

\subsubsection{Informativeness Gaps}\label{igaps}

Look at the following result statement:
\Quote{random sampling is rare} \Art{BalRal22}. %EMSE
If at this point the reader expects the work contains something more
concrete than \Quote{rare}, this is an informativeness gap.
And indeed the article in question contains this information in its
Table 4 and therefore could and should have said
\Pseudoquote{random sampling is rare (8\% of cases)}.

<<>>=
(igap_loc <- raw |>
  summarize(
    icount = max(icount), # max = enough if one coder saw it
    topic  = first(topic),
    .by    = c(citekey, sidx)
  ) |>
  filter(icount > 0) |>
  summarize(n_igap = sum(icount), .by = topic) |>
  mutate(f_igap = n_igap / sum(n_igap)))
(igap_in_results <- igap_loc$f_igap[igap_loc$topic == "result"])
(igap_in_method  <- igap_loc$f_igap[igap_loc$topic == "method"])
@

Informativeness gaps appear mostly
in results statements (\Sexpr{format_percent(igap_in_results)} of the gaps)
or method statements (\Sexpr{format_percent(igap_in_method)} of the gaps).
Most (if not all) of them could be filled by a number.
They tend to cluster,
like here:
\Quote{The results show that PRINS can process large logs much faster
  than a publicly available and well-known state-of-the-art tool,
  without significantly compromising the accuracy of inferred models.} \Art{ShiBiaBri22} %EMSE
This sentence has three informativeness gaps.
A better formulation could have been:
\Pseudoquote{The results show that PRINS can often process logs an order of magnitude faster
  than the well-known state-of-the-art tool MINT,
  but never lost more than 7 percentage points of balanced accuracy.}!!!

<<>>=
(r_igap1 <- r_stat("n_igap_yes"))
(r_igap1_range <- r_range("n_igap_yes"))

(r_igap3 <- r_stat("n_igap_yes3"))
(r_igap3_range <- r_range("n_igap_yes3"))
@

More than half of all abstracts (\Sexpr{r_igap1} \Sexpr{r_igap1_range}) have an informativeness gap
(making them \emph{improper})
and \Sexpr{r_igap3} \Sexpr{r_igap3_range} have three or more.
See the leftmost two groups of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for details.

\Plotwide{nonzerofractionbar_xletgroups_missinginfofractions}{%
	What fraction of abstracts has the following uninformative types of formulations?\\
	One-or-more or three-or-more informativeness gaps (i.e. missed opportunities for being more specific).\\
	Some sentence that only announces (instead of describing) a method, result, conclusion, or
	possible future research.\\
	One or more understandability gaps.\\
	Informativeness gaps are epidemic, worse(!) for structured abstracts.
	Announcing is generally not rare, in particular for results, and tends to be less pronounced
	at IST.
	Not explaining key terms is not rare and worst at EMSE.\\
	\Describegroups}

\subsubsection{Announcements}\label{announcements}

Occasionally, a sentence will not merely miss to report some specific piece of information
but rather fail to provide any useful information at all and merely hint at
information to be found in the article body.
We call such a sentence an announcement.
\Todo{Should we cross-reference back to the respective methods section here? And elsewhere as well? Lloyd: Yes, I think so.}

Example (method announcement from \Art{BesMarBos22}):
\Quote{As a second step, this study sets out to specifically
  provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based
  on an encouraging mindset and attitude from both managers and technical roles to understand how, when and by
  whom such strategies are adopted in practice.} 
Here is what the sentence could have said instead:
\Pseudoquote{We then surveyed 26 managers and 46 technical people, followed by clarifying interviews
  with 4 managers and 2 developers in order to understand how the managers perceive how they are encouraging
  developers to manage technical debt and how the developers perceive the encouragement they receive.}
% based on BasMarBos Sections 3.2.1, 3.2.3, Table 2

Example (results announcement from \Art{FliChaDye22}): %EMSE
\Quote{Finally we provide guidelines/best practices for researchers utilizing time-based data
from Git repositories.} 
This should have been a result statement such as the following:
\Pseudoquote{We provide 6 guidelines. For instance, cutting off all commits before 2014
  will get rid of about 98\% of all bad commits.}

<<>>=
(r_announce       <- r_stat("n_announce_yes"))
(r_announce_range <- r_range("n_announce_yes"))
@

Announcements are a waste of space and a nuisance for the reader,
yet \Sexpr{r_announce} \Sexpr{r_announce_range}
of all abstracts have at least one and we consider those \emph{improper}.
See groups three to six of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for details.


\subsection{How Not To: Undefined Important Terms}\label{ugaps}

It is normal that the reader of an abstract has only a fuzzy understanding
of what certain terms in the abstract mean.
In a good abstract, the \emph{approximate} meaning of a statement using such a term
still comes across.
Sometimes, however, this is not the case:
The uncertainty regarding the meaning of the term becomes so large
that the sentence containing it becomes incomprehensible.
We call such a term use an understandability gap and consider such abstracts to be improper.

Here is an example from \Art{HamMetQas22}:
\Quote{The results of evaluating the generality of the iContractML 2.0 reference model show
  that it is 91.7\% lucid and 72.2\% laconic.} 
Neither of the terms ``lucid'' or ``laconic'' have been introduced before, so that
this sentence has two understandability gaps and
an average reader is not able to make sense of this statement
which represents half of the work's results.

<<>>=
(ucount_venue <- df_c |>
  summarize(
    n = n(),
    n_ugap = sum(ucount >= 1),
    r_ugap = n_ugap / n,
    .by = venue)
)

(venue_worst_ugap <- ucount_venue |> slice_max(r_ugap))
@

<<>>=
(r_ugap       <- r_stat("n_ugap_yes"))
(r_ugap_range <- r_range("n_ugap_yes"))
@

See the rightmost group of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for how frequent this is:
About \Sexpr{r_ugap} \Sexpr{r_ugap_range} of all abstracts have one or more understandability gaps.
The issue is most pronounced at \Sexpr{venue_worst_ugap$venue}.


\subsection{How Not To: Ambiguous Formulations}\label{ambiguity}

Codings with an \Cb{-ignorediff} marker indicate cases where the coders could not agree
despite discussion: the respective sentence (or sentence part) is so highly ambiguous that
more than one role for it is similarly likely.
Obviously, such ambiguous formulations do not represent good abstract writing
and we consider such abstracts to be \emph{improper}.

<<>>=
(n_igdf <- raw |> filter('ignorediff' == code) |>
    summarize(n = n_distinct(citekey, sidx)))
(n_igdf_a <- raw |> filter(code == 'ignorediff') |>
  summarize(n = n_distinct(citekey)))
(r_igdf_a <- n_igdf_a / n_abstracts)

(igdf_cocodes <- raw |>
  filter("ignorediff" %in% code, .by = c(citekey, sidx)) |>
  filter(code != "ignorediff") |>
  count(code) |>
  arrange(desc(n)))

(n_igdf_c <- nrow(igdf_cocodes))

(igdf_cocodes_top5 <- igdf_cocodes |>
  slice_max(n, n = 5, with_ties = FALSE))
(r_igdf_cocodes_top5 <- sum(igdf_cocodes_top5$n)/sum(igdf_cocodes$n))
@

In our data, we found \Sexpr{n_igdf} such cases overall, spread over \Sexpr{n_igdf_a} different abstracts,
so that \Sexpr{format_percent(r_igdf_a)} of all abstracts have one or more such problems.
A total of \Sexpr{n_igdf_c} different codes are involved.
% The most frequent 5 of them cover \Sexpr{format_percent(r_igdf_cocodes_top5)} of the total:
% \Sexpr{igdf_cocodes_top5$code}.


\subsection{How Not To: Summary}\label{hownotto_summary}

Summing up, a proper abstract should be \emph{complete}, i.e., it should provide at least 
a minimal amount of information of types  \Cb{background} (\Cb{or gap}),
\Cb{objective}, \Cb{methods}, \Cb{results}, and \Cb{conclusion}, the
canonical IMRAD structure.

<<>>=
r_complete

(r_complete_unstruc <- filter(df, !is_struc) |>
  summarize(n = n(), n_complete = sum(is_complete),
    r_complete = n_complete / n))
(r_complete_design <- filter(df, is_design) |>
  summarize(n = n(), n_complete = sum(is_complete),
    r_complete = n_complete / n))
@

However, as we see in Figure~\ref{nonzerofractionbar_xletgroups_totalqualityfractions},
a majority of abstracts fails this very basic quality criterion, a depressing result.
\Todo{Decide after in-Python aggregation is settled whether it's a "majority"}

<<>>=
r_proper
(r_proper_range <- r_range("n_proper_yes"))

(proper_venue <- df |>
  summarize(
    n = n(),
    n_proper = sum(is_proper),
    r_proper = n_proper / n,
    .by = venue)
)

(proper_struc <- df |>
  summarize(
    n = n(),
    n_proper = sum(is_proper),
    r_proper = n_proper / n,
    .by = is_struc)
)
@
\Todo{Check formulations in caption of Figure \ref{nonzerofractionbar_xletgroups_totalqualityfractions}
after in-Python aggregation is fix.}

\Plot{nonzerofractionbar_xletgroups_totalqualityfractions}{%
	What fraction of abstracts is \emph{complete} in the sense of 
	Sections~\ref{statisticalevaluation} and \ref{hownotto_summary}?
	What fraction of abstracts is \emph{proper} in the sense of 
	Section~\ref{statisticalevaluation}?\\
    Only \Sexpr{r_complete} \Sexpr{r_complete_range} of abstracts are \emph{complete}. 
	Only about \Sexpr{format_percent(r_complete_unstruc$r_complete)} of unstructured abstracts and
  \Sexpr{format_percent(r_complete_design$r_complete)} of design article abstracts are complete.
	Structured abstracts are much better and therefore all four supposedly top-quality venues
	are beat by the much lower-regarded IST which requires structured abstracts.\\
	Only \Sexpr{r_proper} \Sexpr{r_proper_range} of abstracts are \emph{proper}.
	Here, too, structured abstracts are better than unstructured ones.
	Our TOSEM sample has not a single proper abstract.
	After that, ICSE is worse than all remaining subgroups considered.
	\Describegroups}

Our moderately stricter criterion of being \emph{proper} involves 
being \emph{complete}, having acceptable Flesch-Kincaid readability (>20),
and having neither informativeness gaps nor understandability gaps or highly ambiguous sentences.
Given this list of criteria, improper abstracts will happen from time to time
even for careful authors.
Nevertheless, we believe that a majority of abstracts could and should be \emph{proper} in this sense.
Yet what we find is that only \Sexpr{r_proper} of them indeed are.
% TOSEM and ICSE are still worse than that.
We expected to find many problems in abstracts but still find this outcome astonishingly bad. 

<<results="as.is">>=
stats_overview |>
  filter(!name %in% c("n_igap_yes3")) |>
  mutate(across(starts_with("r_"), format_percent)) |>
  mutate(
    name  = case_match(
      name,
      "n_bg_over_third" ~ "background > $\\frac{1}{3}$",
      .default = stringr::str_extract(name, "[^_]+$")
    ),
    value = ifelse(value %% 1 == 0,
      paste0(as.integer(value), "~~"),
      as.character(value)),
    value = cell_spec(value, escape = FALSE),
    n_range = ifelse(!is.na(min), glue("[{min}, {max}]"), ""),
    r_range = ifelse(!is.na(min), glue("[{r_min}, {r_max}]"), ""),
  ) |>
  select(name, value, n_range, r_value, r_range) |>
  kbl(
    col.names = c(), 
    align = c("l", "r@{\\hspace{2pt}}", "r", "r@{\\hspace{2pt}}", "r"),
    booktabs = TRUE, 
    escape = FALSE,
    label = "overview",
    caption = "For each coding-based quality criterion, there is a possibility for disagreement between the two codings. Numbers in brackets show the extent of that disagreement: For a negative criterion (italics),
  the lower value denotes the number of abstracts for which \\emph{both} codings lead a negative evaluation, the higher value denotes the number of abstracts for which \\emph{at least one} coding leads to a negative evaluation."
  ) |>
  kable_styling() |>
  add_header_above(c("Criterion" = 1, "Count" = 2, "Ratio" = 2), bold = TRUE) |>
  row_spec(0, bold = TRUE) |>
  pack_rows("Readability (Section \\ref{result-len-read})", 1, 3, escape = FALSE) |>
  pack_rows("Innefficient Allocation of Space (Section \\ref{inefficient_allocation})", 4, 4, escape = FALSE) |>
  pack_rows("Completeness (Section \\ref{missing_elements})", 5, 6, escape = FALSE) |>
  pack_rows("Informativeness Gap (Section \\ref{igaps})", 7, 8, escape = FALSE) |>
  pack_rows("Announcements (Section \\ref{announcements})", 9, 10, escape = FALSE) |>
  pack_rows("Understandability Gap (Section \\ref{ugaps})", 11, 12, escape = FALSE) |>
  pack_rows("Ambiguous Formulations (Section \\ref{ambiguity})", 13, 14, escape = FALSE) |>
  pack_rows("Properness (Section \\ref{hownotto_summary})", 15, 16, escape = FALSE) |>
  row_spec(c(3, 6, 8, 10, 12, 14, 16), italic = TRUE)
@


\subsection{How To: Structured Abstracts are more orderly}\label{structuredgood}

This study is mostly exploratory.
Our only clear expectation at the start was that structured abstracts would be
somehow better (whatever that was going to mean) than unstructured ones.
Therefore, we perform two statistical hypothesis tests here that compare
structured to unstructured abstracts.

<<>>=
sig.test <- import_from_path("qabs.printstats", path = "../script")
sig.complete <- sig.test$calc_test(df, "is_complete")
sig.proper <- sig.test$calc_test(df, "is_proper")
@

We find that structured abstracts are significantly more often \emph{complete}
($\chi^2=\Sexpr{sig.complete$chi2 |> round(1)}, \Sexpr{sig.complete$p |> format_p_value()}$).
For \emph{properness}, the absolute numbers are so low that statistical significance
is not achieved
($\chi^2=\Sexpr{sig.proper$chi2 |> round(1)}, \Sexpr{sig.proper$p |> format_p_value()}$)
despite the trend visible in the
relative difference of the two green bars in the right half of
Figure~\ref{nonzerofractionbar_xletgroups_totalqualityfractions}.
Nevertheless, overall the expectation appears to be correct.


%========================================================================
\section{Limitations and Threats to Validity}

\subsection{Interpretivist Perspective}

In terms of Tracy's quality criteria for qualitative research \cite{Tracy10},
our study has nice properties, because our readers inhabit
the domain of abstract reading and abstract writing themselves.

The topic's worthiness is obvious, the amount of data is large,
as is the number of constructs used.
Challenges are discussed below.
Hopefully, our constructs and findings resonate with you;
they certainly resonate with us.
Should you find our description insufficiently thick,
you can easily look up lots of additional examples in our raw data
described in Section~\ref{dataavailability}.


\subsection{Positivist Perspective}

Internal validity is the degree to which the stated method
was followed correctly.
We do not expect much problem in this regard.
The most difficult-to-avoid problem in our study is lapses of concentration
during coding which result in coding mistakes.
However, the laborious coding procedure
described in Section~\ref{meth_coding}
makes it very unlikely for such mistakes to slip through.
Most other steps were automated, so mistakes would be systematic
and not likely to escape our attention.

Construct validity is the degree to which the design
of the study is adequate for the phenomenon to be understood.
Here, our study has obvious limitations:
It ideally ought to measure the informativeness and understandability of
abstracts. However, both of these are reader-dependent, so
measuring them would involve a reading study with many readers.
This would face huge problems in getting a representative set of readers,
could never scale to the hundreds of abstracts we look at here,
and, whichever operationalization it chose,
it would be imperfect and controversial.
We therefore decided to analyze properties of abstracts
that are arguably problematic, although we cannot say just how
problematic in each case, resulting in count statistics only.

External validity is the degree to which the results generalize
to other sets of abstracts:
Here, we expect that our results generalize well to
neighboring (past and future) years in the same venues
we studied, perhaps a bit less for ICSE because of its
varying location and hence more varying authors.
Whether it also generalizes to other venues
we cannot know, but we would be surprised if venues of lower
scientific reputation had articles with better abstracts.


%========================================================================
\section{Conclusions}


\subsection{Too Few Abstracts Have Good Quality}

<<>>=
r_complete
r_proper
r_no_conclusion
@

Only \Sexpr{r_complete} of the investigated software engineering research article abstracts are 
\emph{complete} according to the IMRAD structure long established for abstracts in science
(Figure~\ref{nonzerofractionbar_xletgroups_totalqualityfractions}).
In particular, about half of all abstracts
(\Sexpr{format_percent(r_no_conclusion$r_missing)}) never formulate a conclusion in the sense of
a generalizing take-home message (Figure~\ref{zerofractionbar_xletgroups_topicmissingfractions}).
Only \Sexpr{r_proper} of the abstracts are \emph{complete}
and also fulfill modest additional criteria of 
informativeness and understandability 
(Figures~\ref{boxplots_fkscore} and \ref{nonzerofractionbar_xletgroups_missinginfofractions}, 
Section~\ref{ambiguity}).
We define and quantify further quality issues not included in the above number 
(Figures~\ref{box_xletgroups_topicfractions}, \ref{box_xletgroups_conclusionfractions}, 
\ref{nonzerofractionbar_xletgroups_missinginfofractions}, 
Sections~\ref{murkyobjectives} and \ref{convoluted}).
This low abstracts quality exists despite the fact that we analyzed only venues
supposed to have high quality.

This is deplorable, because for most readers of a typical article, 
the abstract is all they will ever look at.\Todo{reference needed}
With the above level of abstracts quality, they will get far less 
well informed than they could have been and the spread of knowledge
will be slowed down accordingly, needlessly wasting public money.

We conclude the software engineering research community should pay
more attention to abstract-writing.
Presumably, introducing a structured abstract format 
and accompanying writing instructions that suit 
engineering research would help.


\subsection{How To: Guidelines for Well-Written Abstracts}\label{howto-guidelines}

\subsubsection{For Authors}

The steps cross-reference the article sections that supply 
detail or evidence.

\begin{enumerate}
  \item Write a structured abstract, not a free-flowing one 
    (see Section~\ref{structuredgood}).
    Take care to avoid announcements (see Section~\ref{announcements}),
    understandability gaps (see Section~\ref{ugaps}),
    and sentences with an unclear role (see Section~\ref{ambiguity}).
    Provide helpful detail, perhaps simplified, if it consumes barely any space
    (see Section~\ref{igaps}).
  \item Write a \Sah{Background} section that provides just enough context
    and motivation to understand the subsequent \Sah{Objective} 
    (see Section~\ref{inefficient_allocation}).
  \item Decide on your main contribution. 
    Is your article a design article or an empirical one?
    Write an \Sah{Objective} that expresses both succinctly 
    (see Sections \ref{design} and \ref{murkyobjectives}).
    If your background information contains a corresponding \Sah{Gap} statement,
    mark it as such.
  \item If your work is a design article, 
    write a \Sah{Design} section.
    Cover all key ideas (typically two to four), avoid non-key information.
  \item If you have multiple near-independent empirical sub-studies and
    your work is a design article,
    use two or three combined \Sah{Method and Results} sections.
    Each of these will typically be a single sentence of the form
    ``We do-this-and-that and find this-and-that.''
    (see Section~\ref{convoluted}).
    Otherwise write separate \Sah{Methods} and \Sah{Results} sections
    as follows.
  \item Write a \Sah{Methods} section that explains what you have done
    for your empirical study. 
    Be as specific as you can do concisely.
    In particular, mention the amount of data used (see Section~\ref{igaps}).
  \item Write a \Sah{Results} section that explains the main outcomes
    of your study.
    If you have many results, report the one or two most important ones.
    If you have many results of equal importance, report the 
    one or two most interesting ones or just provide examples.
    Be specific and beware of announcements (see Section~\ref{announcements}).
  \item Write a \Sah{Conclusion} section that generalizes from your results.
    What should the reader take home?
    What do we now know that we did not know before?
    The broader the conclusion, the higher the relevance of your work,
    but the lower its credibility. 
    Find a formulation with relevance and good enough credibility.
    Expect to later be proven wrong for some of your works, but not for many
    (see Section~\ref{inefficient_allocation}).
  \item An outlook on future possibilities \emph{can} be part of your \Sah{Conclusion},
    but is usually better left to the body of your article.
\end{enumerate}

\Todo{Take a good short design work abstract and show it in this format?}

\subsubsection{For Venues}

Structured abstracts are not currently used widely in software engineering.
In their usual form, which does not suit design articles, 
this is understandable, even appropriate.

In the above extended form, however, 
(allowing \Sah{Gap} statements, 
allowing \Sah{Design} sections, 
allowing multiple \Sah{Methods and Results} sections) 
structured abstracts promise better abstracts quality than a free-style format
because our results strongly suggest that a structured format is helpful for 
completeness. Previous results show that it is also helpful for understandability.
Therefore, venues should require structured abstracts in our new,
engineering-ready format.

For your call for papers, feel free to copy the above text,
point your readers to the online version,\footnote{\url{https://github.com/serqco/qabstracts/blob/main/rec_abs_structure.md}}
or point authors to this article for the underlying evidence.


%========================================================================


\subsection*{Acknowledgments}
\noindent We thank Gesine Milde for cleansing the automatically extracted abstract texts.

\bibliographystyle{IEEEtran}
\bibliography{special.bib}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{lutz-prechelt.jpg}}]{Lutz Prechelt}
received a PhD from the University of Karlsruhe
for work that combined machine learning and compiler construction
for parallel machines.
He then moved to empirical software engineering and performed
a number of controlled experiments before spending three
years in industry as an engineering manager and CTO.
He is now full professor for software engineering at
Freie Universit√§t Berlin and executive director of the
Institute for Informatics there.
His research interests concern the human
factor in the software development process, asking mostly
exploratory research questions and addressing them with
qualitative methods.
Additional research interests concern research methods
and the health of the research system.
He is the founder of the 
Software Engineering Research Quality Coalition (SERQco)
and the inventor of Review Quality Collector (RQC).
Contact him at prechelt@inf.fu-berlin.de.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{lloyd-montgomery.jpg}}]{Lloyd Montgomery}
is a PhD student at the University of Hamburg, working under Prof. Dr. Walid Maalej.
Currently, his primary research area is the quality of issue tracking systems, with other interests such as empirical software engineering, non-technical factors in software engineering, and science communication.
He won the RE'17 best paper award for his machine learning design science research with IBM customer support.
Lloyd's academic service record includes serving as the IST Publicity Chair, RE'23 Publicity Chair, NLP4RE'22 Workshop Co-Chair, and RE'21 Artefact Co-Chair.
He is on the program committees of REFSQ, NLP4RE, and the RE@Next! track at RE, and regularly reviews for the journals REJ, IST, and JSS, in addition to reviews for SQ Journal and TOSEM.
Lloyd also has a particular passion for artefact tracks, having served as a PC member for seven artefact tracks at RE, ICSE, and FSE.
Contact him at lloyd.montgomery@uni-hamburg.de.
\end{IEEEbiography}  

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{julian-frattini.jpg}}]{Julian Frattini}
is a PhD student at the Department of Software Engineering at Blekinge Institute of Technology (BTH). 
His research under the supervision of Prof. Daniel Mendez focuses on requirements quality, aiming to operationalize the concept of "good-enough requirements engineering." 
Julian served as the proceedings chair of the 30th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'24) and publicity chair of the 32nd IEEE International Requirements Engineering conference (RE'24), and is co-organizing the 8th International Workshop on Crowd-Based Requirements Engineering (CrowdRE'24) and the Eleventh International Workshop on Artificial Intelligence and Requirements Engineering (AIRE'24) at RE'24. 
He is on the program committees of REFSQ, International Conference on the Quality of Information and Communications Technology (QUATIC), and NLP4RE, and regularly reviews for the Requirements Engineering journal (REJ), the Information and Software Technology journal (IST), and the Journal of Systems and Software (JSS).
\end{IEEEbiography}

%\begin{IEEEbiographynophoto}{Jane Doe} 
%Biography text here without a photo.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
%In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}

\ifarxiv{\input{appendix}}

\end{document}


