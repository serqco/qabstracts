% Article about SE abstracts quality (qabstracts) for IEEE Trans. on Software Engineering
% TODO:
% - decide which part of figure discussion belongs into caption and which into the body text
% - https://bjoern.brembs.net/2016/01/even-without-retractions-top-journals-publish-the-least-reliable-science/

\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{cite}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\usepackage{stfloats}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {../img} {.} }
\usepackage{xcolor}
\usepackage{balance}
\hyphenation{semi-conduc-tor IEEE-Xplore}

\newcommand{\ifarxiv}[1]{#1}  % activate this for the ArXiv version of the document
%\newcommand{\ifarxiv}[1]{}  % activate this for the journal version of the document

\newcommand{\Plot}[2]{%
	\begin{figure}[htb]%
		\centering\includegraphics[width=\columnwidth]{#1}%
		\vspace{-4mm}\caption{#2}\label{#1}%
	\end{figure}}

\newcommand{\Plotwide}[2]{%
	\begin{figure*}[tbp]%
		\centering\includegraphics[width=\textwidth]{#1}%
		\vspace{-4mm}\caption{#2}\label{#1}%
    \end{figure*}}

\newcommand{\Cb}[1]{\bgroup\scshape #1\egroup}  % typeset a code name
\newcommand{\Prg}[1]{\bgroup\ttfamily #1\egroup}  % typeset commands and script names
\newcommand{\Art}[1]{\bgroup[#1]\egroup} % typeset article references  TODO: use!
\newcommand{\Sah}[1]{\bgroup\scshape #1\egroup}  % typeset a structured abstract heading  TODO: use!

\newcommand{\Todo}[1]{\textcolor{red}{\sffamily ((#1))}}
\newcommand{\Quote}[1]{\bgroup\itshape ``#1''\egroup}  % quote from real abstract
\newcommand{\Pseudoquote}[1]{\bgroup\itshape ``#1''\egroup}  % quote from fictive abstract
\newcommand{\QuoteCB}[1]{\Quote{#1}}  % quote from codebook
\newcommand{\Describeboxplots}{The box shows the 25-to-75 percentile, 
	the whiskers are 10- and 90-percentile,
	the grey bar is the median,
	the fat dot is the mean.}
\newcommand{\Describegroups}{The plots in each group show these different subsets of abstracts:
	all, structured, non-structured, design, empirical, EMSE, ICSE, IST, TOSEM, TSE.}

\begin{document}

\title{How (Not) To Write a\\Software Engineering Abstract}

\author{Annie Author1, Bert Author2
\thanks{A. Author1 is with A University, Btown, Ccountry}
\thanks{B. Author2 is with D University, Etown, Fcountry}}

\markboth{IEEE Transactions on Software Engineering}%
{How (Not) To Write a Software Engineering Abstract}

\maketitle

\begin{abstract}  % 100 to 200 words are allowed (but not enforced)
\emph{Background:}
Abstracts are a very valuable element in a software engineering research article,
but not all abstracts are as informative as they could be.
\emph{Objective:}
Characterize the structure of abstracts in high-quality venues,
observe and quantify deficiencies,
suggest guidelines for writing informative abstracts.
\emph{Methods:}
Use open coding to derive concepts that explain relevant properties of abstracts
and identify the archetypical structure of abstracts.
Use quantitative content analysis to objectively characterize abstract structure
over a sample of 500!!! abstracts from five presumably high-quality venues.
Use exploratory data analysis to find recurring issues in abstracts.
Compare the prototypical structure to actual structures to derive
guidelines for producing informative abstracts.
\emph{Results:}
!!!
\emph{Conclusions:}
(1)~Even in top venues, many abstracts are far from ideal.
(2)~Structured abstracts tend to be better than unstructured ones,
but (3)~artifact-centric works need a different structured format.
(4)~The community should start requiring conclusions that generalize,
which currently are often missing.
\end{abstract}

\begin{IEEEkeywords}
!!!.
\end{IEEEkeywords}


%========================================================================
\section{Introduction}
\IEEEPARstart{A}{lthough} the abstract is a super important part of any research article~\cite{Lang22},
when reading an abstract in software engineering, even in a presumably top-quality venue,
we often feel it is lacking important information or find it difficult to understand at all.

The present article aims at substantiating this impression.


%Ideally, an abstract should
%situate the work within software engineering,
%motivate and state the issue addressed by it,
%describe the methods applied,
%report empirical results,
%and formulate a useful take-home message.
%With the possible exception of discussing related work,
%this is much the same agenda as for the article itself,
%just with much less detail;
%getting it right is no easy task.


\subsection{Research Questions}

% we use the numbers only in this subsection and the next;
% everywhere else we repeat the question or paraphrase it.
\noindent
We ask several questions:
\begin{enumerate}
	\item What does a typical well-written abstract look like?
	\item Which deficiencies occur?
	\item How often?
	\item Do structured abstracts have better quality than unstructured ones?
	\item How should software engineering abstracts be written?
\end{enumerate}
Of these, numbers 1 and 2 should be considered exploratory,
number 4 is hypothesis-driven (we expect a yes), and
the answer to number 5 we consider a conclusion from the answers to the other four.


\subsection{Research Approach}

No firm expectations regarding questions 1 and 2 exist for engineering articles,
so qualitative methods will have to be used for them:
We start our research by open coding~\cite[Ch. 5]{StrCor90} in order to derive a vocabulary (a set of concepts or codes)
by which the nature of a particular abstract can be characterized.

We intend to convince even people who are skeptical of qualitative research
regarding our answers to questions 3, 4, and 5 and regarding the need to improve
the quality of software engineering abstracts.
Therefore, we continue the research by performing a repeatable
quantitative content analysis~\cite[Ch. 7]{Krippendorff04} for number 3
on a large sample (!!!) of presumably high-quality abstracts
and apply an elaborate seven-step approach for maximizing its reliability.

The final stage is the statistical evaluation of the content analysis data,
which is again exploratory and is straightforward in both
the questions it asks and the statistical methods it uses for answering them.


\subsection{Research Contributions}

Our contributions correspond to the research questions as follows:
\begin{enumerate}
	\item As for the structure of well-written abstracts, we present an ``abstract archetype''
	  that describes fixed parts of the structure and degrees of freedom.
	\item We describe and discuss !!! types of deficiencies.
	\item We quantify the frequency of each deficiency type for the entire sample of abstracts
	  as well as for 9 different subgroups of interest.
	\item We present convincing data that structured abstracts tend to be better
	  in several respects.
	\item We provide data-based how-to instructions for abstracts writing for authors
	  as well as guidance for editors and conference organizers.
	  Software Engineering works should use structured abstracts, but
	  need a different and more flexible template than what is used so far.
\end{enumerate}


%========================================================================
\section{Related Work}

There is a considerable literature on research abstracts across disciplines
and we will not attempt to summarize it here.
Instead, we only provide examples of the different major perspectives of those studies
and otherwise focus on what has been done in the software engineering domain.


\subsection{Abstracts Structure}

Swales introduced ``genre analysis'' as a means for teaching academic reading and writing
especially to non-native speakers~\cite{Swales90}:
Genres are text types;
genre analysis means deconstructing texts (from a genre) to better understand
their elements in
terms of their syntactical structure, content, role, and interrelationships
(e.g., their relative position within the whole).

For our purposes here, the most relevant idea from genre analysis is the notion
of ``moves'', which are, roughly speaking, the building blocks used by the writers
for making their overall point.
Several studies have looked at the move structure of research abstracts
in different fields such as
applied linguistics~\cite{DosSantos96} or
protozoology~\cite{CroOpp06}.
Despite the differences of research content, they find very similar moves,
such as this five-move structure~\cite{CroOpp06}:
\begin{quote}
	(1)~situate the research within the scientific community;
	(2)~introduce the research by describing the main features or presenting its purpose;
	(3)~describe the methodology;
	(4)~state the results;
	(5)~draw conclusions or suggests practical applications.
\end{quote}
This reflects, in slightly extended form, the IMRAD structure 
(Introduction, Methods, Results, and Discussion) of the body of scientific articles
that has gradually become the norm since the 1940s \cite{SolPer04}.

We found a similar structure for abstracts of 
empirical works in software engineering (see Section~\ref{archetype}),
but abstracts of artifact-centric works (tool building) do not fit this model and need
an extended one.


\subsection{Abstracts Quality}\label{relatedworkquality}

Several studies on abstracts focus on quality assessment,
most often in subfields of the biomedical domain.
Many such studies cover articles of a homogeneous nature:
all randomized controlled trials (controlled experiments).
This allows formulating very specific expectations what information should be
presented in an abstract and allows performing the analysis in checklist fashion,
for example:
In clinical dermatology, \cite{DupKhoLeb03} used a 30-item checklist on 197 abstracts
  for computing a 0-to-1 score and found mean scores between 0.64 and 0.78 for their various subgroups.
In dental medicine, \cite{ShaHar06} used a 29-item checklist on 100 abstracts
  and found a mean score of only 0.54.
  % suggests an 8-move structure for structured abstracts:
  % objective, design, setting, patients, interventions, outcome measure, results, conclusion.
  % per-move deficit rates are not reported.
Among 303 abstracts of cost-effectiveness analyses,
  29\% did not report the baseline to which the intervention
  had been compared \cite{RosGreSto05}.
Among 146 abstracts of meta-analyses in peridontology, 
  33\% did not even report the direction in which  
  % it is indeed 32.9% percent: (146-15-83)/146, Table 2
  the evidence was pointing \cite{FagLiuHud14}.
  
Various studies have investigated ``spin'' in the context of significance testing.
The term covers two types of behavior: 
Using language that sounds more positive than warranted or
reporting a secondary or alternative statistic as if it was the main one of interest.
Among all abstracts reporting non-significant results, studies found spin in 
  45\% of abstracts of orthopedic controlled experiments (\cite{ArtZaaChe20}),
  44\% in emergency medicine (\cite{ReyRidBro20}),
  56\% in psychiatry and psychology (\cite{JelRobBow20}),
  and 58\% for the conclusions alone across a broad set of 
    medical controlled experiments (\cite{BouDutRav10}).

Unfortunately, the methods of those studies are not applicable to a 
broad sample of software engineering works, because
in fields with heterogeneous study structures such as ours, the operationalization of
\emph{quality} is less straight-forward.
For example, spin can take many more forms in software engineering articles
than is assumed by the studies mentioned above and it is difficult
to decide which forms are acceptable and which are not.

One approach for discussing the quality of a software engineering abstract 
could be through comparing to a known ``good'' structure for abstracts.
For instance~\cite{CroOpp06} remarks
that one third of the 12 analyzed abstracts is lacking move 2 (stating a purpose).


\subsection{Structured vs.\ Unstructured Abstracts}\label{relatedworkstructured}

Many studies of abstracts quality do not study quality in general.
For instance neither~\cite{DupKhoLeb03} nor~\cite{ShaHar06} reports
which of their checklist items are missing most frequently.
Rather, their research question is the relative quality of structured
versus unstructured abstracts.
And in almost all of those cases, including~\cite{DupKhoLeb03} and~\cite{ShaHar06},
the answer is: structured abstracts have fewer quality issues.

A structured abstract is one that uses a prescribed sequence of 
intermediate headings, such as Background, Objective, Methods, Results,
Conclusions or some similar set.

Such research can be highly influential.
For instance, the CONSORTS report~\cite{MohHopSch12}
(containing guidelines for reporting controlled experiments, with over 10,000 citations),
relies on such a study \cite{HarSydBlu96} to recommend structured abstracts.
% https://journals.sagepub.com/doi/epdf/10.1177/016555159602200503

In software engineering, Kitchenham proposed
Evidence Based Software Engineering (EBSE) in 2004 \cite{KitDybJor04}.
EBSE relies a lot on Systematic Literature Reviews (SLR).
The practicality of SLRs hinges on the informativeness of abstracts:
Can the researcher decide quickly and reliably, whether the present article
belongs in the study or not?

Therefore, Kitchenham performed two studies on structured abstracts
in software engineering.
The first took 23 published non-structured abstracts, 
converted them into structured ones,
and compared the two versions. 
It found that the structured abstracts were much longer,
but also had much better readability scores \cite{KitBreOwe08}.

The second, by Budgen, Kitchenham, and others \cite{BudKitCha08},
is a controlled experiment based on similar pairs of abstracts
rewritten into the structure 
Background, Aim, Methods, Results, Conclusions.
20 students and 44 researchers and practitioners
each judge one structured and one different unstructured abstract
for completeness (using an 18-item checklist) and 
clarity (using a vague 1-to-10 scale).
The structured format was found to increase the 
completeness score by 6.6 and the clarity score by 3.0.
70\% of the subjects also preferred the structured format
subjectively.

Both studies use only abstracts of purely empirical studies,
not tool-building works, 
which have very different (and more complicated) properties
as we will see.



%========================================================================
\section{Methods}


\subsection{Overview}

Our study is a full-blown content analysis in the sense of Krippendorff~\cite{Krippendorff04}:
not just a counting exercise with a fixed codebook,
but rather an iterative codebook development before (and during) the counting
and an extensive abductive inference exercise after the counting.

It can be conceptualized as consisting of four widely overlapping stages or phases
as shown in Figure~\ref{qabstracts_timeline_commits}:
%
\Plot{qabstracts_timeline_commits}{%
	Timeline of the main study phases and their individual events.
    Each character's x-coordinate represents the time of a git commit.
    The vertical scattering is added for legibility only.}
%
Codebook development, which started first and is described in Sections~\ref{meth_codingrules}
and \ref{meth_codebook},
defined the rules and target concepts of the counting.
Training was for developing a joint understanding of the codebook and also contributed
greatly to the codebook's early evolution;
it is described in Section~\ref{meth_training}
Coding worked on a large sample of abstracts from top-quality venues described in
Section~\ref{meth_sample} and produced the count data subsequently used in the statistical analysis.
Coding is described in Section~\ref{meth_coding}.
!!!Statistical evaluation

Overall, we consider our study to be a qualitative one, but note that the middle two elements,
coding and statistical evaluation,
are compatible with a positivist epistemology (aiming for ``objective'' results) so that
the final interpretation is done on a solid quantitative foundation.


\subsection{Data Availability}\label{dataavailability}

We publish not only the outcome of our study, but also most parts of its development and
execution history in full detail: as a git version repository.
It includes all versions of the
codebook, handling procedure, coded abstracts, Python scripts for automation,
Python scripts for tabulations and plots, and the manuscript of this article.
Find it at \url{https://github.com/serqco/qabstracts/}.


\subsection{General Coding Rules}\label{meth_codingrules}

Besides the definition of the content categories (codes),
our codebook contains global rules for the coding that can be summarized as follows:
we code by sentence;
we prefer single codes per sentence over multi-codings;
for choosing a code, we act like readers and consider only what we have seen before plus
one sentence forward context when needed and only when needed;
be friendly and avoid coding negative properties whenever an alterative, more positive
interpretation is plausible as well.


\subsection{Codebook and Codebook Development}\label{meth_codebook}


\subsubsection{\Cb{background, objective, method, result, conclusion}}

The codebook was initialized with concepts for the five sections commonly used
in a structured abstract:\footnote{In Krippendorff's terminology, this is an ``established theories'' justification
of analytical constructs~\cite[Section 9.2.3]{Krippendorff04}.}
\Cb{background, objective, method, result, conclusion}.
These represent, respectively: context information, the study goal or question,
empirical approach, empirical outcomes, and a take-home message that generalizes beyond the results.

Each code is defined by a short verbal explanation.
For example the definition of \Cb{method} reads
\QuoteCB{information about the approach or setup of an empirical (or possibly purely mathematical) study.}.
The initial definitions were made more and more precise and unambiguous by later
codebook refinements.
For example for \Cb{method}, the part \QuoteCB{(or possibly purely mathematical)} was initially not present
and was added when we encountered the first of those (very few) purely mathematical studies
during the coding process and were confused which code was appropriate for some sentence.
To help disambiguation, a few of the definitions need to be much longer than the above.


\subsubsection{Artifact-centric Studies: \Cb{design}}\label{design}

In a phase internally called ``prestudy'', the codebook was then refined and extended
based on coding attempts for a stratified sample of 20 abstracts from ICSE 2021.
% we used every 7th article, which constitutes stratification by virtue of the
% topic-centric session blocks in which the program is arranged.
We quickly recognized that additional codes were
needed.\footnote{In Krippendorff's terminology, this is an ``expert knowledge and experience'' justification
    of analytical constructs~\cite[Section 9.2.2]{Krippendorff04}:
    Being software engineering researchers ourselves, 
    we recognize when a sentence makes a different kind
    of contribution to an abstract than can be described by existing codes,
    and we are able to define what kind of contribution it is.}
Most importantly,
many software engineering articles do not predominantly talk about
an empirical study.
Their focus is the design of some artifact, most often a tool,
sometimes a method or something else.
Much of the abstract is then spent on design considerations, design decisions,
techniques applied in implementation, and so on.
Such artifact-centric studies, although they also usually contain an empirical study,
are very different in nature from purely empirical studies;
we therefore introduced the code \Cb{design} to mark such material.
\Cb{design} has the longest definition of all our codes: 170 words.

We call the articles that contain at least one \Cb{design} coding in their abstract
``design works'', the others ``empirical works''.


\subsubsection{Refinements: \Cb{gap, summary, fposs} etc.}

At various later points during the work, we recognized a need for more granular coding
in order to capture differences in abstract writing we wanted to measure.
This led to the splitting of existing codes and the introduction of additional ones.
We then reworked existing codings to use the new codes consistently throughout.

The most important cases of such additional codes are these:
\Cb{gap} sentences state what is unknown or not yet possible;
\Cb{summary} sentences summarize several results, but do not provide new information.
In contrast to a \Cb{conclusion}, a summary statement does not generalize beyond the immediate results.
\Cb{fposs}, \Cb{fneed}, \Cb{fwork} sentences occur at the end of abstracts.
They state what future work is now possible, needed, or planned-by-the-authors.

% TODO: examples of gap, summary, fposs, conclusion


\subsubsection{Subjective Additions: \Cb{:i, :u, :hype}}

% TODO: Maybe move those additions to the end, to separate objective and subjective codes?

The codes described so far aim at codifying repeatable properties,
where several well-trained coders will come to the same result with high probability.
In addition, we defined a number of suffixes for codes, by which coders can provide
additional information for which the expectation of agreement is much lower.
These are also not neutral, like the codes themselves, but all describe some
kind of deficiency.
The most important of these suffixes are the following:

Informativeness gaps are spots in a sentence where the coder desired to know
additional detail that is presumably available to the authors and
that can presumably be provided in very little space.
Example (from \Art{LiuFenYin22}):
\Quote{To evaluate DeepState, we conduct an extensive empirical study on popular datasets
and prevalent RNN models containing image and text processing tasks.}
This sentence was coded as \Cb{method:i2}, because the coder asked themselves
\Quote{How many datasets? How many RNN models?}
The answers are both given in that article's Table 3: Four datasets, three models.
The authors could and should have given that information in the abstract.

Understandability gaps are spots in a sentence where the coder finds their usual intuitive
half-understanding of a term used in the abstract insufficient for understanding
the abstract overall.

\Todo{example of :u1}

The \Cb{:hype} suffix marks statements that praise the article far more
than warranted, often by using exaggerated adverbs such as ``extremely''.


\subsubsection{Codes for Announcements: \Cb{a-*}}

Sometimes, statements do not merely have informativeness gaps,
they provide no concrete information at all.
We call such statements announcements (because their presence insinuates there will
be information about this in the article body) and provide extra codes for them.

For example, here is an \Cb{a-method} (method announcement) from \Art{BesMarBos22}:
\Quote{As a second step, this study sets out to specifically
  provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based
  on an encouraging mindset and attitude from both managers and technical roles to understand how, when and by
  whom such strategies are adopted in practice.}
So many words, yet we learn nothing about how
the assessment
or the analysis work.

Here is an \Cb{a-result} (results announcement) from \Art{FlyChaDye22}:
\Quote{Based on those results, we then used the Boa and Software
  Heritage infrastructures to help identify and quantify several sources of dirty Git timestamp data.}
The first part is \Cb{method}, the second should have been \Cb{result},
but, alas, we learn nothing about those sources' nature or number or impact.


\subsubsection{Codes for Headings: \Cb{h-*}}

We also have codes for the headings used in structured abstracts.
These codes are conceptual, i.e., for instance
\Quote{Aim:}, \Quote{Goal:}, \Quote{Objective:}, \Quote{Question:}, and their plural forms
would all be coded as \Cb{h-objective}.
Likewise, there are \Cb{h-background}, \Cb{h-method}, and so on.
We recognize structured abstracts by the presence of such a heading code.


\subsection{Training}\label{meth_training}

The training phase (internally called ``prestudy2'') served two purposes:
Finding/repairing deficiencies in the codebook,
and arriving at a joint interpretation of it across the four coders
(the four authors\footnote{Four more people were involved in the training phase at some
  point but did not eventually join the study.})
As you can see in Figure~\ref{qabstracts_timeline_commits},
the training phase extended over almost half a year and triggered
the majority of the codebook improvements.

In the training phase, we perfected the mechanics of the coding process
described below, in particular the very useful \Prg{compare-codings} script
and email routine,
and generally formed as a research team.


\subsection{Sample}\label{meth_sample}

We decided not to aim for a broad selection of all software engineering research,
but rather concentrate on what is presumably the highest quality material:
The ICSE technical research track,
the three journals allowed for journal-first presentations at ICSE
(Empirical Software Engineering EMSE,
ACM Transactions on Software Engineering and Methodology TOSEM,
IEEE Transactions on Software Engineering TSE).
Since we expected to find that structured abstracts had better quality
than unstructured ones, we added a fifth venue that required structured abstracts:
Information and Software Technology (IST, an Elsevier journal).
IST has published many very good systematic literature reviews and methods works,
but is not \emph{generally} considered a top-quality venue.

We wanted to draw a random sample of 100 articles per venue from the
2022 volumes, but found that TOSEM has published only 86 articles per year,
so we ended up at 486 articles initially.
A few of those later had to be removed because they were other things,
often editorials.
Furthermore, we eventually did not need quite as much data for answering our
questions and stopped coding after !!! abstracts.
Still, ours is the largest manual study of abstracts we know of.

Volume downloading, sampling, and abstract extraction into publishable
and annotation-ready text files were all done automatically by the
retrievelit\footnote{\url{https://github.com/serqco/retrievelit/}},
select-sample, and prepare-sample scripts, respectively.


\subsection{Coding Process}\label{meth_coding}

% TODO Explain the self-selection of blocks and the slight steering ("More Franz/Julian combinations, please");
%      characterize the eventual distribution of blocks to coders.

We code each abstract twice, by so-called coders A and B.
Abstracts are held in text files in separate directories \Prg{abstracts.A} and \Prg{abstracts.B}.
Each sentence is followed by a line containing \Prg{\{\{\}\}} and into this pair of
double braces the coder would enter their codings,
such as \Prg{\{\{method,result:i2\}\}} for a complex sentence that contains substantial
amounts of method information as well as results with two informativeness gaps.
We batch the coding in blocks of 8 abstracts each.
The procedure, coordinated via git, is best explained by example,
which we do in the following two subsections.


\subsubsection{Coding}

\indent Step 1. When Lloyd wanted to code a block of abstracts on 2023-05-26,
he found the next available block to be Block 17 B.
(Lutz had coded Block 17 A previously.)
Lloyd reserved the block in the coordination file \Prg{sample-who-what.txt}
and performed the coding.

Step 2. He then ran \Prg{check-codings} to test his codings
against the codebook and corrected any mistakes, such as typos.

Step 3. He then ran \Prg{compare-codings} to compare his codings against Lutz'.
This script creates one \emph{report block} for each sentence where the codings
of coders A and B are not compatible.
Compatible means: Differing at most in the subjective suffixes \Cb{:i} and \Cb{:u}, but not in the codes
(and not by more than one in the numbers of informativeness gaps and understandability gaps).
If Lloyd found a report block where Lutz' coding was obviously correct and his own
obviously wrong, he would simply correct his coding.

Step 4. He would then commit his coded abstracts into git.


\subsubsection{Handling Disagreements}

% TODO What about Step 5a?

\indent Step 5b. For the remaining report blocks, Lloyd would write an email to Lutz explaining his reasoning.

Step 6. Lutz would read through that email and categorize the report blocks into the following cases:\\
a) Lloyd's coding is obviously correct, Lutz' own is a clerical error.
Lutz would correct his coding and respond accordingly.
Figure~\ref{email-MeyAlmKel22.png} shows such a case.\\
b) Lutz finds Lloyd's coding clearly incorrect.
He would respond with an explanation why he thinks so.\\
c) Lutz finds Lloyd's coding acceptable, but his own coding preferable.
He would respond with an explanation of his reasoning and suggest that Lloyd
either adjust his coding or add an \Cb{-ignorediff} marker to it.
Semantically, this suffix indicates two accepted alternative interpretations of the same sentence.
Programmatically, it silences the \Prg{compare-codings} script for this particular report block,
so that the coding difference is now officially accepted.\\
d) Lutz finds his own coding acceptable, but Lloyds preferable.
He would either adjust his coding or add an \Cb{-ignorediff} marker to it
and respond accordingly.\\
e) Lutz finds both codings equally acceptable.
He would add an \Cb{-ignorediff} marker to his own
and respond accordingly, explaining is reasoning.

Step 7. Lutz would commit his corrected abstracts and send the response email.

Step 8. Lloyd would read the response email and usually act on it to finish the handling of
Block 17.
Only very rarely would he disagree with something to a degree that would make
another round of emails necessary.

\begin{figure*}[tbp]%
	\centering\fbox{\includegraphics[width=0.95\textwidth]{email-MeyAlmKel22.png}}%
	\vspace{-2mm}\caption{Excerpt from Lutz' response email during the disagreements handling
		for Block 17:
		Report block from the \Prg{compare-codings} script at the top,
		text line from Lloyds first email in the middle,
		Lutz' response below.
		This one was a simple case;
		sometimes several lines of text are provided by each coder.
		Overall, Lloyd's first email contained report blocks for 9 disagreements
		in 4 abstracts, both typical numbers.}\label{email-MeyAlmKel22.png}%
\end{figure*}


\subsubsection{Effect of this Procedure}

Most content analyses code most material only once,
then code a random subsample twice,
compute some coefficient of agreement,
report it as a measure of good-enough coding quality,
and that's that.

In contrast, our above-described procedure has two effects:
\begin{enumerate}
	\item It maximizes the quality of the coding.
	  Very few clerical errors will have managed to escape our discussion process
	  and the definitions of all codes that are sometimes difficult to tell apart
	  have been refined until they were very mature.
	\item It finds all those spots in the sample where the abstracts are so convoluted
	  or strangely formulated that even our careful code definitions do not lead
	  to a canonical judgment.
	  These spots should clearly be considered to be badly written -- which is great,
	  because that is what our study is interested in.
\end{enumerate}
Note that, technically speaking, our procedure also leads to a 100\% perfect inter-coder agreement,
because even the cases of \Cb{-ignorediff} indicate that
the coders agree on multiple plausible interpretations of one sentence.


\subsection{Statistical Evaluation}

The difficult part of our statistical evaluation is asking the right questions;
our data provides a lot of possibilities.
In contrast, the actual statistical techniques are simple and straightforward:
Mostly tabulations of counts or percentages, bar plots, and box plots.

We mostly refrain from performing significance tests or computing confidence intervals,
because most analyses are exploratory, not driven by specific expectations or theories.
Also, most phenomena we report are gradual by nature.
The one exception from this rule is the comparison of
structured versus unstructured abstracts!!!.

In this spirit, we often use the following verbal terms for frequencies:
rare (less than 5\%),
not rare (5-20\%),
common (20-35\%),
frequent (35-50\%),
dominant (over 50\%).!!!


\subsection{Interpretation}

!!!Driven by research interest; grounded as far as possible.
Makes use of impressions not captured in the data.
Choice of examples reflects those.


\subsection{Use of Examples}

We will use examples from real abstracts from our sample to illustrate
some of our statements.
We identify their source by a citation key
formed from three letters each of the first three authors' names.
For example article 1 in block 1 in our study was written by
Fregnan, Petrulio, Di Geronimo, and Bacchelli
and would be identified as \Art{FrePetGer22}.

We select those examples for the clarity of the phenomenon in question,
not for the abstract's quality.
For the source of every positive example there are others that are
as good or better.
For the source of every negative example there are others that are
as bad or worse.
Since the use of an example is not about the article it stems from,
we do not include those articles in our references list.

%========================================================================
\section{Results}

\subsection{Length and Readability}

Typical abstracts (the middle half) are 200 to 290 words long
and 8 to 14 sentences long.
Structured abstracts tend to be longer than unstructured ones.
\ifarxiv{See Figures \ref{boxplots_words} to \ref{boxplots_avg_wordlength} for details.}
The official word limits of
150--250 for EMSE,
maximum 300 for IST, and
recommended maximum 250 for TSE
are disobeyed by more than a quarter of all articles for each of these three venues.

The Flesch-Kincaid ``reading ease'' readability score \cite{KinFisRog75}
is a validated and widely used metric that judges readability of English text
based only on the number of words per sentence and the number of
syllables per word.
Values under 30 represent graduate-level difficulty,
under 10 extreme difficulty for native speakers.
Considering that most community members are not native speakers of English,
we judge 20 to 30 to be a range suitable for researcher audiences and so
we call anything over 30 ``good'',
20 to 30 ``normal'', and
under 20 ``overly difficult''.

See Figure~\ref{boxplots_fkscore} for the results:
only about 10\% of all abstracts have good readability,
less than 40\% have normal readability,
and a majority is overly difficult --- one quarter is even below 10!
ICSE is a bit better than the other venues,
TOSEM is worse.

\Plot{boxplots_fkscore}{%
	Flesch-Kincaid 'reading ease' readability score, higher is better.
    Values under 50 are considered difficult to read (college-level material).
    Under 30: very difficult to read (graduate level, acceptable for abstracts).
	Under 10: extremely difficult to read (overly difficult).
    Differences between subgroups are modest.
	ICSE is best (mean 20), TOSEM is worst (mean 13).
    Structured abstracts are just as difficult as nonstructured ones
    if one ignores the ``Methods:'' etc. headings as we did here.
}


\subsection{Design Articles vs. Empirical Articles}

We described the idea of the \Cb{design} code in Section~\ref{design}.
But many empirical works involve some artifact design as well,
so how is the discrimination made?
That depends on how the authors phrase their goal in their
\Cb{objective} statement.
Consider the following goal statements:

\Todo{Find 2 examples}

The first puts the artifact at the center, so this is a design work.
The second puts empirical results at the center, so this is an empirical work
and the \Cb{design} code will not be used for them.
Artifact design discussion will then usually be coded as \Cb{method} instead.
Mixed cases are rare and then the coders will decide which aspect has more weight.

In design works, a large fraction of the abstract will be devoted to
describing design considerations for that artifact;
in our data: typically 16\% to 34\% of the words.
The empirical study, which usually exists as well, then usually has a mere
supporting role (validating the claims made in the artifact discussion)
and is correspondingly given less space:
7\%--15\% (versus 12\%--23\%) for description of empirical method,
12\%--24\% (versus 18\%--32\%) for description of empirical results.


\subsection{The Abstracts Archetype}\label{archetype}

Compared to the content structure assumed by the usual formats of
structured abstracts, our codebook is more fine grained;
the \Cb{design} code is but one example.

During our sensemaking process, we had a number of insights regarding
how a well-written abstract ``ticks'', which we eventually distilled into
the following template, which we call the software engineering abstracts archetype:

\begin{enumerate}
\item An abstract consists of three parts, in this order:
   \emph{Introduction}, \emph{Study Description}, and \emph{Outlook}.
\item Two turning points connect the three parts:\\
   a) A statement of the study goals (\Cb{objective}) connects \emph{Introduction}
      to \emph{Study Description}.\\
   b) A generalizing statement ("take-home message", \Cb{conclusion})
     connects \emph{Study Description} to \emph{Outlook}.
\item The \emph{Introduction} first introduces the topic area of the study and what is known (\Cb{background})
   and then may or may not point out a gap in knowledge (\Cb{gap}).
\item For an empirical article, the \emph{Study Description} begins with
   method description (\Cb{method}), followed by results description (\Cb{result}).
   Sometimes, this sequence occurs twice in a row, very rarely more.
\item For a design article, design description (\Cb{design}, see below) precedes the structure
   described in the previous point.
\item Occasionally (but infrequently), \emph{Study Description} will end with a study summary (\Cb{summary}).
\item After the \Cb{conclusion}, the \emph{Outlook} talks about future research and states
   what could now be done (\Cb{fposs}, for future possibilities),
   what should now be done (\Cb{fneed}),
   what the authors themselves intend to do (\Cb{fwork}), or
   what is still not known (\Cb{fgap}).
   Several statements of each type may occur, in no particular order.
\end{enumerate}

The archetype describes all variants of abstracts that have a natural train of thought,
so any deviation will lead to a less easily understandable abstract.

\Todo{Find short, good example}


\subsection{How Not To: Inefficient Allocation of Space}\label{inefficient_allocation}

If one reads those abstracts, one can hardly help notice that some of them 
spend a lot of space on \Cb{background}, although its only purpose is to situate
the objective and make it understandable.
Here is an example from an article titled
``!!!'':
\begin{quote}
  \Todo{Does an example pay off here?}	
\end{quote}	

Other abstracts manage to solve the same problem in a wonderfully concise manner:
\begin{quote}
	\Todo{Find example}	
\end{quote}	

\Plotwide{box_xletgroups_topicfractions}{%
	Per-topic distribution of the amount of space used for that topic.\\
	\Describeboxplots
	\Describegroups}

\Plot{box_xletgroups_conclusionfractions}{%
	Comparison of the amounts of remaining space devoted to the conclusion for abstracts
	with a rather short background section (lowest quarter) vs those
	with a long one (highest quarter).
	The latter conclusions are shorter even though the indicated percentage 
	pertains to the part of the abstract after the background only.\\
	\Describegroups}

As we can see in the Background group of boxplots in Figure~\ref{box_xletgroups_topicfractions},
some venues have a quarter of articles that spend one third or more of the abstract space
on background. This leads to deficiencies later on, as we can see in
Figure~\ref{box_xletgroups_conclusionfractions}:
The conclusion is the potentially most useful part of the abstract,
the take-home message, 
but with a long background section, it tends to become pronouncedly shorter.

Background lengths are more benign for structured abstracts.


\subsection{How Not To: Murky \Cb{objective} statements}\label{murkyobjectives}

!!!


\subsection{How Not To: Missing Elements}\label{missing elements}

Given the archetype, one way to approach an analysis of abstracts quality is to ask
how often key parts of an abstract are missing entirely.
This is shown in Figure~\ref{zerofractionbar_xletgroups_topicmissingfractions}.

\Plotwide{zerofractionbar_xletgroups_topicmissingfractions}{%
	How often is a topic not present at all in an abstract?\\
	\Describegroups}

The often-missing \Cb{gap} and \emph{Outlook} parts \Cb{fposs}, \Cb{fneed}, etc. are clearly optional,
so it is not a problem that they are often not present.
\Cb{background} and \Cb{objective} are hardly ever missing.

\Cb{method} and \Cb{result} are sometimes missing (and this is a problem),
but never in structured abstracts.

The shocking part of this analysis is \Cb{conclusion}, which ought to be present
as the key take-home message in any abstract, but is in fact missing in more than
half of all -- yet again very rarely in structured abstracts\footnote{Note that
  the prompt of having to write something after a "Conclusion:" keyword
  alone cannot guarantee this:
  The statement put there can be (and sometimes is) something else,
  typically a \Cb{result} statement or \Cb{summary}.}


\subsection{How Not To: Convoluted Trains of Thought}\label{convoluted}

Leaving out important parts from an abstract is not the only way for wrecking
the abstract's value, however.
There are also abstracts where the train-of-thought is so convoluted
that they become difficult to read.

For quantifying this, we map each abstract to a short sequence of letters,
where each letter stands for a contiguous stretch of sentences in the abstract
that have the same content type; the letter designates that content type.

For instance, an abstract that follows a minimal incarnation of the Archetype
for an empirical article would be encoded as bomrc, which stands for
the content types sequence background, objective, methods, results, conclusion.

The full list of abstracts structures is too long to show it here.
It has 95!!! entries for empirical articles and 122!!! entries for design articles.
Most of these have only one or two instances, though;
we restrict ourselves to the types that occur with higher frequency here.
Note that a frequency of e.g. 2.5 simply means that the two coders did not agree.
The results are shown in
Figure~\ref{ab_topicstructure_freqs_empir} for empirical articles and
Figure~\ref{ab_topicstructure_freqs_design} for design articles.

\Plot{ab_topicstructure_freqs_empir}{%
  The frequency of different trains-of-thought in the abstract for empirical articles.
  The label is a string of stretch-code characters:
  b-ackground, g-ap, o-bjective, d-esign, m-ethod, r-esult, s-ummary, c-onclusion, O-utlook.}
\Plot{ab_topicstructure_freqs_design}{%
  The frequency of different trains-of-thought in the abstract for design articles.
  Same characters as before, except that d can now in fact occur.}

\subsubsection{Empirical Articles}

Let us start with the simpler empirical articles.
We see that the most sensible structures are also the most frequent (bars 1, 2, 3):
Archetypical with or without a \Cb{gap} statement, perhaps with an \emph{Outlook}.
But from bar 4 on, the abstracts are more or less pathological:
The first few are mostly missing a conclusion, later on (not shown in the figure)
follow all kinds of curious structures such as, to pick an admittedly extreme case,
bgomrcsrsc: two conclusions, two summaries, summary after conclusion.

\subsubsection{Design Articles}

It naturally gets worse for the design articles with their more complicated structure:
Even the top two structures are missing a conclusion
and we find complex repetitive structures starting at bar 5.
Not all of these are automatically bad.
In particular, having two small empirical studies, resulting in a nice bgodmrmrc structure,
can make a lot of sense for the reader.
But this has its limits: An abstract with structure bodmrmrmrmr is overloaded.
It can also easily get out of hand, as confirmed by the examples bobdrmrmr and bomdmrmrmrsmrc.

\subsubsection{Structured Abstracts}

Today's conventions for structured abstracts may be a bit restrictive
(e.g. by not accommodating the useful mrmr substructure),
but they do result in more orderly abstracts:
There are only 27!!! different abstract structures for the structured abstracts of empirical articles
versus 95!!! for empirical articles overall.
(!!! replace 95 by mean of same-sized samples)


\subsection{How Not To: Uninformative Formulations}

A milder way of reducing the usefulness of an abstract is using formulations that
fail to provide information that, at this point, would be useful to the reader and
could be provided using only very few additional words.

Our investigation has identified and then quantified two types
of such lacks of informativeness.
We call them informativeness gaps and announcements, respectively.

\subsubsection{Informativeness Gaps}\label{igaps}

Look at the following result statement:
\Quote{random sampling is rare} \Art{BalRal22}. %EMSE
If at this point the reader expects the work contains something more
concrete than \Quote{rare}, this is an informativeness gap.
And indeed the article in question contains this information in its
Table 4 and therefore could and should have said
\Pseudoquote{random sampling is rare (8\% of cases)}.

Informativeness gaps appear mostly in results statements (82\% of the gaps)
or method statements (16\% of the gaps).
Most (but not all) of them could be filled by a number.
They tend to cluster,
like here:
\Quote{The results show that PRINS can process large logs much faster
  than a publicly available and well-known state-of-the-art tool,
  without significantly compromising the accuracy of inferred models.} \Art{ShiBiaBri22} %EMSE
This sentence has three informativeness gaps.
A better formulation could have been:
\Pseudoquote{The results show that PRINS can often process logs an order of magnitude faster
  than the well-known state-of-the-art tool MINT,
  but never lost more than 7 percentage points of balanced accuracy.}!!!

More than half of all abstracts have an informativeness gap
and 18\%!!! have three or more.
See the leftmost two groups of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for details.

\Plotwide{nonzerofractionbar_xletgroups_missinginfofractions}{%
	What fraction of abstracts has the following uninformative types of formulations?\\
	One-or-more or three-or-more informativeness gaps (i.e. missed opportunities for being more specific).\\
	Some sentence that only announces (instead of describing) a method, result, conclusion, or
	possible future research.\\
	One or more understandability gaps.\\
	Informativeness gaps are epidemic, worse(!) for structured abstracts.
	Announcing is generally not rare, in particular for results, and tends to be less pronounced
	at IST.
	Not explaining key terms is not rare and worst at EMSE.\\
	\Describegroups}

\subsubsection{Announcements}\label{announcements}

Occasionally, a sentence will not merely miss to report some specific piece of information
but rather fail to provide any useful information at all and merely hint at
information to be found in the article body.
We call such a sentence an announcement.

Example 1:
\Quote{As a second step, this study sets out to specifically
  provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based
  on an encouraging mindset and attitude from both managers and technical roles to understand how, when and by
  whom such strategies are adopted in practice.} \Art{BesMarBos22}
So many words, so little information in this would-be method statement!
Here is what the sentence could have said instead:
\Pseudoquote{We then surveyed 26 managers and 46 technical people, followed by clarifying interviews
  with 4 managers and 2 developers in order to understand how the managers perceive how they are encouraging
  developers to manage technical debt and how the developers perceive the encouragement they receive.}
% based on BasMarBos Sections 3.2.1, 3.2.3, Table 2

Example 2:
\Quote{Finally we provide guidelines/best practices for researchers utilizing time-based data
from Git repositories.} \Art{FliChaDye22}  %EMSE
This could have been a result statement such as the following:
\Pseudoquote{We provide 6 guidelines. For instance, cutting off all commits before 2014
  will get rid of about 98\% of all bad commits.}

Announcements are a waste of space and a nuisance for the reader,
yet 24\% of all abstracts have at least one.
See groups three to six of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for details.


\subsection{How Not To: Undefined Important Terms}\label{ugaps}

It is normal that the reader of an abstract has only a fuzzy understanding
of what certain terms in the abstract mean.
In a good abstract, the approximate meaning of a statement using such a term
still comes across.
Sometimes, however, this is not the case:
The uncertainty regarding the meaning of the term becomes so large
that the sentence containing it becomes incomprehensible.
We call such a term use an understandability gap.

Here is an example:
\Quote{The results of evaluating the generality of the iContractML 2.0 reference model show
  that it is 91.7\% lucid and 72.2\% laconic.} \Art{HamMetQas22}
Neither of the terms "lucid" or "laconic" have been introduced before, so that
this sentence has two understandability gaps and
an average reader is not able to make sense of this statement
which represents half of the work's results.

See the rightmost group of Figure~\ref{nonzerofractionbar_xletgroups_missinginfofractions}
for how frequent this is:
About 17\%!!! of all abstracts have one or more understandability gaps.
The issue is most pronounced at ICSE.


\subsection{How Not To: Ambiguous Formulations}\label{ambiguity}

Codings with an \Cb{-ignorediff} marker indicate cases where the coders could not agree
despite discussion: the respective sentence (or sentence part) is so highly ambiguous that
more than one role for it is similarly likely.
Obviously, such ambiguous formulations do not represent good abstract writing.

In our data, we found 74!!! such cases overall, spread over 30!!! different abstracts,
so that 5\%!!! of all abstracts have one or more such problems.
A total of 14!!! different codes are involved.
The most frequent 5!!! of them cover 61\%!!! of the total:
design, conclusion, result, a-result, and method.

Although a detailed discussion and more examples could be interesting,
we find the phenomenon is not frequent enough to warrant more detail.


\subsection{How-Not-To Summary}\label{hownotto_summary}

Good quality means: Follows archetype, acceptable readability score, no announcement, no gap.
Should large majority (over 75\% of articles).
Indeed x\% no announcement, x\% no gap, x\% follow archetype, x\% have acceptable readability score.
And only x\% have all of these properties.


\subsection{How To: Structured Abstracts are more orderly}\label{structuredgood}

Re-analysis of ``Convoluted Trains of Thought''.

Table: percentage of: all/unstructured/structured have which deficiency,\\
Total: have any deficiency.

x\% no announcement, x\% no gap, x\% follow archetype, x\% have acceptable readability score.
And only x\% have all of these properties.




%========================================================================
\section{Limitations and Threats to Validity}

\subsection{Interpretivist Perspective}

In terms of Tracy's quality criteria for qualitative research \cite{Tracy10},
our study has very nice properties, because our readers inhabit
the domain of abstract reading and abstract writing themselves.

The topic's worthiness is obvious, the amount of data is large,
as is the number of constructs used.
Challenges are discussed below.
Hopefully, our constructs and findings resonate with you;
they certainly resonate with us.
Should you find our description insufficiently thick,
you can easily look up lots of additional examples in our raw data
described in Section~\ref{dataavailability}.


\subsection{Positivist Perspective}

Internal validity is the degree to which the stated method
was followed correctly.
We do not expect much problem in this regard.
The most difficult-to-avoid problem in our study is slips of concentration
during coding which result in coding mistakes.
However, the laborious coding procedure
described in Section~\ref{meth_coding}
makes it very unlikely for such mistakes to slip through.
Most other steps were automated, so mistakes would be systematic
and not likely to escape our attention.

Construct validity is the degree to which the design
of the study is adequate for the phenomenon to be understood.
Here, our study has obvious limitations:
It ideally ought to measure the informativeness and understandability of
abstracts. However, both of these are reader-dependent, so
measuring them would involve a reading study with many readers.
This would face huge problems in getting a representative set of readers,
could never scale to the hundreds of abstracts we look at here,
and, whichever operationalization it chose,
it would be imperfect and controversial.
We therefore decided to analyze properties of abstracts
that are arguably problematic, although we cannot say just how
problematic in each case, resulting in count statistics only.

External validity is the degree to which the results generalize
to other sets of abstracts:
Here, we expect that our results generalize well to
neighboring (past and future) years in the same venues
we studied, perhaps a bit less for ICSE because of its
varying location and hence more varying authors.
Whether it also generalizes to other venues
we cannot know, but we were surprised if venues of lower
scientific reputation had articles with better abstracts.


%========================================================================
\section{Conclusions}


\subsection{...}
\noindent


\subsection{How To: Guidelines for Well-Written Abstracts}

\subsubsection{For Authors}

The steps cross-reference the article sections that supply 
detail or evidence.

\begin{enumerate}
  \item Write a structured abstract, not a free-flowing one 
    (see Section \ref{structuredgood}).
    Take care to avoid announcements (see Section \ref{announcements}),
    understandability gaps (see Section \ref{ugaps}),
    and sentences with an unclear role (see Section \ref{ambiguity}).
    Provide helpful detail, perhaps simplified, if it consumes barely any space
    (see Section \ref{igaps}).
  \item Write a \Sah{Background} section that provides just enough context
    and motivation to understand the subsequent \Sah{Objective} 
    (see Section \ref{inefficient_allocation}).
  \item Decide on your main contribution. 
    Is your article a design article or an empirical one?
    Write an \Sah{Objective} that expresses both succinctly 
    (see Sections \ref{design} and \ref{murkyobjectives}).
  \item If your work is a design article, 
    write a \Sah{Design} section.
    Cover all key ideas (typically two to four), avoid non-key information.
  \item If you have multiple near-independent empirical sub-studies and
    your work is a design article,
    use two or three combined \Sah{Method and Results} sections.
    Each of these will typically be a single sentence of the form
    ``We do-this-and-that and find this-and-that.''
    (see Section \ref{convoluted}).
    Otherwise write separate \Sah{Methods} and \Sah{Results} sections
    as follows.
  \item Write a \Sah{Methods} section that explains what you have done
    for your empirical study. 
    Be as specific as you can do concisely.
    In particular, mention the amount of data used (see Section \ref{igaps}).
  \item Write a \Sah{Results} section that explains the main outcomes
    of your study.
    If you have many results, report the one or two most important ones.
    If you have many results of equal importance, report the 
    one or two most interesting ones or just provide examples.
    Be specific and beware of announcements (see Section \ref{announcements}).
  \item Write a \Sah{Conclusion} section that generalizes from your results.
    What should the reader take home?
    What do we now know that we did not know before?
    The broader the conclusion, the higher the relevance of your work,
    but the lower its credibility. 
    Find a formulation with relevance and good enough credibility.
    Expect to later be proven wrong for some of your works, but not for many
    (see Section \ref{inefficient_allocation}).
  \item An outlook on future possibilities \emph{can} be part of your \Sah{Conclusion},
    but is usually better left to the body of your article.
\end{enumerate}


\subsubsection{For Venues}

Structured abstracts are not currently used widely in software engineering.
In their usual form, which does not suit design articles, 
this is understandable, even appropriate.

In the above extended form, however, structured abstracts promise
better abstracts quality than a free-style format.
Therefore, venues should require structured abstracts in this
engineering-ready format.

For your call for papers, feel free to copy the above text and
either remove the ``see Section'' cross references or
point authors to this article for the underlying evidence.


%========================================================================


\subsection*{Acknowledgments}
\noindent We thank Gesine Milde for cleansing the automatically extracted abstract texts.

\bibliographystyle{IEEEtran}
\bibliography{special.bib}

%\begin{IEEEbiographynophoto}{Jane Doe} 
%Biography text here without a photo.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
%In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}

\ifarxiv{\input{appendix}}

\end{document}


